# Q1

## Moment Generating Function (MGF) of Bernoulli Distribution

A Bernoulli random variable $X$ takes values $1$ with probability $p$ and $0$ with probability $1-p$. The probability mass function (pmf) is given by:

$$
P(X = x) =
\begin{cases} 
p & \text{if } x = 1, \\
1-p & \text{if } x = 0.
\end{cases}
$$

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

Substitute the pmf of $X$ into the expectation:

$$
M_X(t) = \sum_{x \in \{0, 1\}} e^{tx} P(X = x).
$$

Evaluate the sum:

$$
M_X(t) = e^{t \cdot 0}(1-p) + e^{t \cdot 1}p = (1-p) + pe^t.
$$

Thus, the MGF of a Bernoulli random variable is:

$$
M_X(t) = 1-p + pe^t.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X'(t) = \frac{d}{dt} \left( 1-p + pe^t \right) = p e^t.
$$

Evaluate at $t = 0$:

$$
\mu_X = M_X'(0) = p e^0 = p.
$$

Thus, the mean of $X$ is:

$$
\mu_X = p.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X''(t) = \frac{d^2}{dt^2} \left( 1-p + pe^t \right) = \frac{d}{dt}(pe^t) = pe^t.
$$

Evaluate $M_X''(t)$ at $t = 0$:

$$
M_X''(0) = p e^0 = p.
$$

We already computed $M_X'(0) = p$, so:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = p - p^2 = p(1-p).
$$

---

# Q2

## Moment Generating Function (MGF) of Exponential Distribution

An exponential random variable $X$ with rate parameter $\lambda > 0$ has the probability density function (PDF):

$$
f_X(x) =
\begin{cases} 
\lambda e^{-\lambda x}, & \text{if } x \geq 0, \\
0, & \text{if } x < 0.
\end{cases}
$$

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

Substitute the definition of expectation for a continuous random variable:

$$
M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x) \, dx.
$$

For the exponential distribution, the PDF is nonzero only for $x \geq 0$. Substituting the PDF:

$$
M_X(t) = \int_0^\infty e^{tx} \lambda e^{-\lambda x} \, dx.
$$

Simplify the exponent:

$$
M_X(t) = \lambda \int_0^\infty e^{x(t - \lambda)} \, dx.
$$

For the integral to converge, $t < \lambda$ (to ensure $t - \lambda < 0$). Then:

$$
M_X(t) = \lambda \int_0^\infty e^{-x(\lambda - t)} \, dx.
$$

The integral of $e^{-ax}$ is:

$$
\int_0^\infty e^{-ax} \, dx = \frac{1}{a}, \quad \text{for } a > 0.
$$

Substitute $a = \lambda - t$:

$$
M_X(t) = \lambda \cdot \frac{1}{\lambda - t}.
$$

Thus, the MGF of an exponential random variable is:

$$
M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{for } t < \lambda.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X(t) = \frac{\lambda}{\lambda - t}.
$$

Differentiate with respect to $t$:

$$
M_X'(t) = \frac{d}{dt} \left( \frac{\lambda}{\lambda - t} \right) = \frac{\lambda}{(\lambda - t)^2}.
$$

Evaluate at $t = 0$:

$$
\mu_X = M_X'(0) = \frac{\lambda}{\lambda^2} = \frac{1}{\lambda}.
$$

Thus, the mean of $X$ is:

$$
\mu_X = \frac{1}{\lambda}.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X'(t) = \frac{\lambda}{(\lambda - t)^2}.
$$

Differentiate again:

$$
M_X''(t) = \frac{d}{dt} \left( \frac{\lambda}{(\lambda - t)^2} \right) = \frac{2\lambda}{(\lambda - t)^3}.
$$

Evaluate at $t = 0$:

$$
M_X''(0) = \frac{2\lambda}{\lambda^3} = \frac{2}{\lambda^2}.
$$

Now, substitute into the formula for variance:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = \frac{2}{\lambda^2} - \left( \frac{1}{\lambda} \right)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2}.
$$

Simplify:

$$
\text{Var}(X) = \frac{1}{\lambda^2}.
$$

---


# Q3

## Moment Generating Function (MGF) of Gamma Distribution

A gamma random variable $X$ with shape parameter $k > 0$ and rate parameter $\lambda > 0$ has the probability density function (PDF):

$$
f_X(x) =
\begin{cases}
\frac{\lambda^k x^{k-1} e^{-\lambda x}}{\Gamma(k)}, & x \geq 0, \\
0, & x < 0,
\end{cases}
$$

where $\Gamma(k)$ is the gamma function, defined as:

$$
\Gamma(k) = \int_0^\infty t^{k-1} e^{-t} \, dt.
$$

---

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int_{-\infty}^\infty e^{tx} f_X(x) \, dx.
$$

Substitute the PDF of $X$:

$$
M_X(t) = \int_0^\infty e^{tx} \frac{\lambda^k x^{k-1} e^{-\lambda x}}{\Gamma(k)} \, dx.
$$

Combine the exponential terms:

$$
M_X(t) = \frac{\lambda^k}{\Gamma(k)} \int_0^\infty x^{k-1} e^{-(\lambda - t)x} \, dx, \quad \text{for } t < \lambda.
$$

Recognize the integral as the form of the gamma function:

$$
\int_0^\infty x^{a-1} e^{-bx} \, dx = \frac{\Gamma(a)}{b^a}, \quad b > 0.
$$

Here, $a = k$ and $b = \lambda - t$. Thus:

$$
M_X(t) = \frac{\lambda^k}{\Gamma(k)} \cdot \frac{\Gamma(k)}{(\lambda - t)^k}.
$$

Simplify:

$$
M_X(t) = \left( \frac{\lambda}{\lambda - t} \right)^k, \quad t < \lambda.
$$

Thus, the MGF of a gamma random variable is:

$$
M_X(t) = \left( \frac{\lambda}{\lambda - t} \right)^k, \quad t < \lambda.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X(t) = \left( \frac{\lambda}{\lambda - t} \right)^k.
$$

Using the chain rule:

$$
M_X'(t) = k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{\lambda}{(\lambda - t)^2}.
$$

Evaluate at $t = 0$:

$$
M_X'(0) = k \left( \frac{\lambda}{\lambda} \right)^{k-1} \cdot \frac{\lambda}{\lambda^2} = k \cdot \frac{1}{\lambda}.
$$

Thus, the mean of $X$ is:

$$
\mu_X = \frac{k}{\lambda}.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X'(t) = k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{\lambda}{(\lambda - t)^2}.
$$

Differentiate again:

$$
M_X''(t) = \frac{d}{dt} \left[ k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{\lambda}{(\lambda - t)^2} \right].
$$

Using the product and chain rules, we find:

$$
M_X''(t) = k(k-1) \left( \frac{\lambda}{\lambda - t} \right)^{k-2} \cdot \frac{\lambda^2}{(\lambda - t)^4} + k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{2\lambda}{(\lambda - t)^3}.
$$

Evaluate $M_X''(t)$ at $t = 0$:

$$
M_X''(0) = k(k-1) \cdot \frac{\lambda^2}{\lambda^4} + k \cdot \frac{2\lambda}{\lambda^3}.
$$

Simplify:

$$
M_X''(0) = \frac{k(k-1)}{\lambda^2} + \frac{2k}{\lambda^2}.
$$

Combine terms:

$$
M_X''(0) = \frac{k^2 + k}{\lambda^2}.
$$

Now compute the variance:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = \frac{k^2 + k}{\lambda^2} - \left( \frac{k}{\lambda} \right)^2.
$$

Simplify:

$$
\text{Var}(X) = \frac{k^2 + k}{\lambda^2} - \frac{k^2}{\lambda^2} = \frac{k}{\lambda^2}.
$$

---

# Q4

## Moment Generating Function (MGF) of Normal Distribution

A normal random variable $X$ with mean $\mu$ and variance $\sigma^2 > 0$ has the probability density function (PDF):

$$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \quad x \in \mathbb{R}.
$$

---

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

Substitute the definition of expectation for a continuous random variable:

$$
M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x) \, dx.
$$

Substitute the PDF of $X$:

$$
M_X(t) = \int_{-\infty}^\infty e^{tx} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \, dx.
$$

Combine the exponential terms in the integrand:

$$
M_X(t) = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^\infty e^{-\frac{1}{2\sigma^2} \left[ (x-\mu)^2 - 2\sigma^2 tx \right]} \, dx.
$$

Simplify the term in the exponent:

$$
(x-\mu)^2 - 2\sigma^2 tx = x^2 - 2\mu x + \mu^2 - 2\sigma^2 tx.
$$

Factor $x$:

$$
x^2 - 2(\mu + \sigma^2 t)x + \mu^2.
$$

Complete the square for $x$:

$$
x^2 - 2(\mu + \sigma^2 t)x + (\mu + \sigma^2 t)^2 - (\mu + \sigma^2 t)^2 + \mu^2.
$$

Rewrite the exponent:

$$
-\frac{1}{2\sigma^2} \left[ (x - (\mu + \sigma^2 t))^2 - (\mu + \sigma^2 t)^2 + \mu^2 \right].
$$

Separate terms:

$$
M_X(t) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{(\mu + \sigma^2 t)^2 - \mu^2}{2\sigma^2}} \int_{-\infty}^\infty e^{-\frac{(x - (\mu + \sigma^2 t))^2}{2\sigma^2}} \, dx.
$$

The integral is the Gaussian integral for a shifted normal distribution, and it evaluates to 1. Thus:

$$
M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}.
$$

Differentiate with respect to $t$:

$$
M_X'(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \left( \mu + \sigma^2 t \right).
$$

Evaluate at $t = 0$:

$$
M_X'(0) = e^{\mu \cdot 0 + \frac{\sigma^2 \cdot 0^2}{2}} \cdot \mu = \mu.
$$

Thus, the mean of $X$ is:

$$
\mu_X = \mu.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X'(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \left( \mu + \sigma^2 t \right).
$$

Differentiate again:

$$
M_X''(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \left( \mu + \sigma^2 t \right)^2 + e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \sigma^2.
$$

Evaluate $M_X''(t)$ at $t = 0$:

$$
M_X''(0) = e^{\mu \cdot 0 + \frac{\sigma^2 \cdot 0^2}{2}} \cdot \left( \mu^2 + \sigma^2 \right) = \mu^2 + \sigma^2.
$$

Now compute the variance:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = \mu^2 + \sigma^2 - \mu^2.
$$

Simplify:

$$
\text{Var}(X) = \sigma^2.
$$

---

# Q5

## Given Information

The joint density function of $X$ (minimum) and $Y$ (maximum) for two independent uniform random variables on $[0, 1]$ is:

$$
f(x, y) = 2, \quad 0 \leq x \leq y \leq 1.
$$

---

### Part (a) Covariance and Correlation of $X$ and $Y$

#### Covariance
We have already calculated:

$$
\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = -\frac{7}{36}.
$$

---

#### Correlation
The correlation is:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}.
$$

1. **Variance of $X$:**

$$
\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
$$

From earlier calculations:

$$
\mathbb{E}[X^2] = \frac{1}{6}, \quad (\mathbb{E}[X])^2 = \left(\frac{2}{3}\right)^2 = \frac{4}{9}.
$$

$$
\text{Var}(X) = \frac{1}{6} - \frac{4}{9} = \frac{3}{18} - \frac{8}{18} = -\frac{5}{18}.
$$

2. **Variance of $Y$:**

Similarly:

$$
\text{Var}(Y) = \mathbb{E}[Y^2] - (\mathbb{E}[Y])^2.
$$

$$
\mathbb{E}[Y^2] = \int_0^1 y^2 f_Y(y) \, dy = \int_0^1 y^2 \cdot 2y \, dy = 2 \int_0^1 y^3 \, dy.
$$

$$
\mathbb{E}[Y^2] = 2 \cdot \frac{1}{4} = \frac{1}{2}.
$$

$$
\text{Var}(Y) = \frac{1}{2} - \frac{4}{9} = \frac{9}{18} - \frac{8}{18} = \frac{1}{18}.
$$

3. **Correlation:**

$$
\text{Corr}(X, Y) = \frac{-\frac{7}{36}}{\sqrt{-\frac{5}{18} \cdot \frac{1}{18}}}.
$$

This indicates there is negative correlation.

---

### Part (b) Conditional Expectations

1. **Find $\mathbb{E}[X \mid Y = y]$:**

The conditional density $f_{X \mid Y}(x \mid y)$ is:

$$
f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} = \frac{2}{2y} = \frac{1}{y}, \quad 0 \leq x \leq y.
$$

The conditional expectation is:

$$
\mathbb{E}[X \mid Y = y] = \int_0^y x \cdot \frac{1}{y} \, dx = \frac{1}{y} \int_0^y x \, dx = \frac{1}{y} \left[ \frac{x^2}{2} \right]_0^y = \frac{y}{2}.
$$

2. **Find $\mathbb{E}[Y \mid X = x]$:**

Similarly, the conditional density $f_{Y \mid X}(y \mid x)$ is:

$$
f_{Y \mid X}(y \mid x) = \frac{f(x, y)}{f_X(x)} = \frac{2}{2(1-x)} = \frac{1}{1-x}, \quad x \leq y \leq 1.
$$

The conditional expectation is:

$$
\mathbb{E}[Y \mid X = x] = \int_x^1 y \cdot \frac{1}{1-x} \, dy = \frac{1}{1-x} \int_x^1 y \, dy.
$$

Solve the integral:

$$
\int_x^1 y \, dy = \left[ \frac{y^2}{2} \right]_x^1 = \frac{1}{2} - \frac{x^2}{2}.
$$

Substitute:

$$
\mathbb{E}[Y \mid X = x] = \frac{1}{1-x} \cdot \left( \frac{1}{2} - \frac{x^2}{2} \right).
$$

---

### Part (c) PDF of $\mathbb{E}[X \mid Y]$ and $\mathbb{E}[Y \mid X]$

The conditional expectations $\mathbb{E}[X \mid Y = y] = \frac{y}{2}$ and $\mathbb{E}[Y \mid X = x]$ are deterministic functions, so their PDFs are degenerate (Dirac delta functions) at $\frac{y}{2}$ and $\frac{1}{2(1-x)}(1-x^2)$, respectively.

---

### Part (d) Linear Predictor of $Y$ in terms of $X$

We seek a linear predictor $\hat{Y} = a + bX$ that minimizes the mean squared error (MSE):

$$
\text{MSE} = \mathbb{E}[(Y - \hat{Y})^2].
$$

From linear regression theory, the coefficients $a$ and $b$ are given by:

$$
b = \frac{\text{Cov}(X, Y)}{\text{Var}(X)}, \quad a = \mathbb{E}[Y] - b\mathbb{E}[X].
$$

Using earlier results:

$$
b = \frac{-\frac{7}{36}}{-\frac{5}{18}} = \frac{7}{10}, \quad a = \frac{2}{3} - \frac{7}{10} \cdot \frac{2}{3}.
$$

Simplify $a$:

$$
a = \frac{2}{3} - \frac{14}{30} = \frac{2}{3} - \frac{7}{15} = \frac{10}{15} - \frac{7}{15} = \frac{3}{15} = \frac{1}{5}.
$$

Thus, the linear predictor is:

$$
\hat{Y} = \frac{1}{5} + \frac{7}{10}X.
$$

The minimal MSE is:

$$
\text{MSE} = \text{Var}(Y) - b^2 \text{Var}(X).
$$

$$
\text{MSE} = \frac{1}{18} - \left(\frac{7}{10}\right)^2 \cdot \left(-\frac{5}{18}\right).
$$

---

### Part (e) Predictor $\hat{Y} = h(X)$ Minimizing MSE

The optimal nonlinear predictor $\hat{Y} = \mathbb{E}[Y \mid X]$:

$$
\hat{Y} = \mathbb{E}[Y \mid X = x] = \frac{1}{1-x} \left( \frac{1}{2} - \frac{x^2}{2} \right).
$$

The minimal MSE is the residual variance:

$$
\text{MSE} = \text{Var}(Y \mid X).
$$

Since $\mathbb{E}[Y \mid X]$ is deterministic, $\text{Var}(Y \mid X) = 0$.

---
# Q6

## Sketching Contours and Regression Lines for a Bivariate Normal Distribution

### Problem Statement
The bivariate normal distribution has $\mu_X = \mu_Y = 0$, $\sigma_X = \sigma_Y = 1$, and the correlation $\rho$. We need to:
1. Sketch the contours of the joint density.
2. Plot the regression lines $\mathbb{E}[Y \mid X = x]$ and $\mathbb{E}[X \mid Y = y]$ for $\rho = 0, 0.5, 0.9$.

---

### Joint Density Function
The joint PDF of a bivariate normal distribution is:

$$
f(x, y) = \frac{1}{2\pi \sqrt{1-\rho^2}} \exp \left(-\frac{1}{2(1-\rho^2)} \left[ x^2 - 2\rho xy + y^2 \right] \right).
$$

#### Key Properties of the Contours:
1. The contours of $f(x, y)$ are ellipses, centered at $(0, 0)$.
2. The tilt of the ellipse depends on the correlation $\rho$:
   - If $\rho = 0$, the ellipses are circles (no correlation).
   - As $\rho$ increases, the ellipses stretch along the line $y = x$.

---

### Regression Lines
The conditional expectations $\mathbb{E}[Y \mid X = x]$ and $\mathbb{E}[X \mid Y = y]$ are linear:
1. $\mathbb{E}[Y \mid X = x] = \rho x$
2. $\mathbb{E}[X \mid Y = y] = \rho y$

For $\rho = 0, 0.5, 0.9$:
- $\rho = 0$: The regression lines are flat (no correlation).
- $\rho = 0.5$: Regression lines have slope $0.5$.
- $\rho = 0.9$: Regression lines are steep with slope $0.9$.

---

# Q7

## Covariance, Correlation, and Conditional Expectations

### Problem Statement
The joint density is given by:

$$
f(x, y) = e^{-y}, \quad 0 \leq x \leq y.
$$

We compute:
1. Covariance and correlation of $X$ and $Y$,
2. $\mathbb{E}[X \mid Y = y]$ and $\mathbb{E}[Y \mid X = x]$,
3. PDFs of $\mathbb{E}[X \mid Y]$ and $\mathbb{E}[Y \mid X]$.

---

### Part (a) Covariance and Correlation

#### Marginal Densities
The marginal densities are derived by integrating $f(x, y)$:
1. Marginal of $X$:
   
   $$
   f_X(x) = \int_x^\infty e^{-y} \, dy = e^{-x}.
   $$

3. Marginal of $Y$:

   $$
   f_Y(y) = \int_0^y e^{-y} \, dx = y e^{-y}.
   $$

---

#### Expectations
1. Compute $\mathbb{E}[X]$:

   $$
   \mathbb{E}[X] = \int_0^\infty x f_X(x) \, dx = \int_0^\infty x e^{-x} \, dx = 1.
   $$

1. Compute $\mathbb{E}[Y]$:

   $$
   \mathbb{E}[Y] = \int_0^\infty y f_Y(y) \, dy = \int_0^\infty y^2 e^{-y} \, dy = 2.
   $$

3. Compute $\mathbb{E}[XY]$:

   $$
   \mathbb{E}[XY] = \int_0^\infty \int_0^y xy f(x, y) \, dx \, dy.
   $$

   Solve the inner integral:

   $$
   \int_0^y xy e^{-y} \, dx = y e^{-y} \int_0^y x \, dx = y e^{-y} \cdot \frac{y^2}{2} = \frac{y^3 e^{-y}}{2}.
   $$

   Now integrate over $y$:

   $$
   \mathbb{E}[XY] = \int_0^\infty \frac{y^3 e^{-y}}{2} \, dy = \frac{1}{2} \int_0^\infty y^3 e^{-y} \, dy = \frac{1}{2} \cdot 3! = 3.
   $$

5. Covariance:

   $$
   \text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = 3 - (1)(2) = 1.
   $$

7. Correlation:
   Use:

   $$
   \text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}.
   $$

   Compute variances:
   - $\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = 2 - 1^2 = 1$,
   - $\text{Var}(Y) = \mathbb{E}[Y^2] - (\mathbb{E}[Y])^2 = 6 - 4 = 2$.

   Correlation:

   $$
   \text{Corr}(X, Y) = \frac{1}{\sqrt{1 \cdot 2}} = \frac{1}{\sqrt{2}}.
   $$

---

### Part (b) Conditional Expectations

1. $\mathbb{E}[X \mid Y = y]$:

   $$
   f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} = \frac{e^{-y}}{y e^{-y}} = \frac{1}{y}, \quad 0 \leq x \leq y.
   $$

   $$
   \mathbb{E}[X \mid Y = y] = \int_0^y x \cdot \frac{1}{y} \, dx = \frac{1}{y} \cdot \frac{y^2}{2} = \frac{y}{2}.
   $$

3. $\mathbb{E}[Y \mid X = x]$:

   $$
   f_{Y \mid X}(y \mid x) = \frac{f(x, y)}{f_X(x)} = \frac{e^{-y}}{e^{-x}} = e^{-(y-x)}, \quad y \geq x.
   $$

   $$
   \mathbb{E}[Y \mid X = x] = \int_x^\infty y e^{-(y-x)} \, dy.
   $$

   Solve:

   $$
   \int_x^\infty y e^{-(y-x)} \, dy = x + 1.
   $$

---

# Q8

### Moment-Generating Function of $X$

1. MGF:

   $$
   M_X(t) = \mathbb{E}[e^{tX}] = \int_0^1 e^{tx} 2x \, dx.
   $$

   Solve:

   $$
   M_X(t) = 2 \int_0^1 x e^{tx} \, dx = 2 \left[ \frac{e^{tx}(tx - 1)}{t^2} \right]_0^1 = \frac{2(e^t - 1)}{t^2}.
   $$

3. Verify:
   - $\mathbb{E}[X] = M_X'(0) = \frac{1}{2}$,
   - $\mathbb{E}[X^2] = M_X''(0) = \frac{1}{3}$.

---

# Q9

### Q9: Linear Combination of Normals

Use the MGF to show $Y = \sum \alpha_i X_i$ is normal. The MGF is:

$$
M_Y(t) = \exp \left( t \sum \alpha_i \mu_i + \frac{t^2}{2} \sum \alpha_i^2 \sigma_i^2 \right).
$$
