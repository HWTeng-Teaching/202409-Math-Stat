# Q1

## Moment Generating Function (MGF) of Bernoulli Distribution

A Bernoulli random variable $X$ takes values $1$ with probability $p$ and $0$ with probability $1-p$. The probability mass function (pmf) is given by:

$$
P(X = x) =
\begin{cases} 
p & \text{if } x = 1, \\
1-p & \text{if } x = 0.
\end{cases}
$$

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

Substitute the pmf of $X$ into the expectation:

$$
M_X(t) = \sum_{x \in \{0, 1\}} e^{tx} P(X = x).
$$

Evaluate the sum:

$$
M_X(t) = e^{t \cdot 0}(1-p) + e^{t \cdot 1}p = (1-p) + pe^t.
$$

Thus, the MGF of a Bernoulli random variable is:

$$
M_X(t) = 1-p + pe^t.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X'(t) = \frac{d}{dt} \left( 1-p + pe^t \right) = p e^t.
$$

Evaluate at $t = 0$:

$$
\mu_X = M_X'(0) = p e^0 = p.
$$

Thus, the mean of $X$ is:

$$
\mu_X = p.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X''(t) = \frac{d^2}{dt^2} \left( 1-p + pe^t \right) = \frac{d}{dt}(pe^t) = pe^t.
$$

Evaluate $M_X''(t)$ at $t = 0$:

$$
M_X''(0) = p e^0 = p.
$$

We already computed $M_X'(0) = p$, so:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = p - p^2 = p(1-p).
$$

---

# Q2

## Moment Generating Function (MGF) of Exponential Distribution

An exponential random variable $X$ with rate parameter $\lambda > 0$ has the probability density function (PDF):

$$
f_X(x) =
\begin{cases} 
\lambda e^{-\lambda x}, & \text{if } x \geq 0, \\
0, & \text{if } x < 0.
\end{cases}
$$

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

Substitute the definition of expectation for a continuous random variable:

$$
M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x) \, dx.
$$

For the exponential distribution, the PDF is nonzero only for $x \geq 0$. Substituting the PDF:

$$
M_X(t) = \int_0^\infty e^{tx} \lambda e^{-\lambda x} \, dx.
$$

Simplify the exponent:

$$
M_X(t) = \lambda \int_0^\infty e^{x(t - \lambda)} \, dx.
$$

For the integral to converge, $t < \lambda$ (to ensure $t - \lambda < 0$). Then:

$$
M_X(t) = \lambda \int_0^\infty e^{-x(\lambda - t)} \, dx.
$$

The integral of $e^{-ax}$ is:

$$
\int_0^\infty e^{-ax} \, dx = \frac{1}{a}, \quad \text{for } a > 0.
$$

Substitute $a = \lambda - t$:

$$
M_X(t) = \lambda \cdot \frac{1}{\lambda - t}.
$$

Thus, the MGF of an exponential random variable is:

$$
M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{for } t < \lambda.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X(t) = \frac{\lambda}{\lambda - t}.
$$

Differentiate with respect to $t$:

$$
M_X'(t) = \frac{d}{dt} \left( \frac{\lambda}{\lambda - t} \right) = \frac{\lambda}{(\lambda - t)^2}.
$$

Evaluate at $t = 0$:

$$
\mu_X = M_X'(0) = \frac{\lambda}{\lambda^2} = \frac{1}{\lambda}.
$$

Thus, the mean of $X$ is:

$$
\mu_X = \frac{1}{\lambda}.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X'(t) = \frac{\lambda}{(\lambda - t)^2}.
$$

Differentiate again:

$$
M_X''(t) = \frac{d}{dt} \left( \frac{\lambda}{(\lambda - t)^2} \right) = \frac{2\lambda}{(\lambda - t)^3}.
$$

Evaluate at $t = 0$:

$$
M_X''(0) = \frac{2\lambda}{\lambda^3} = \frac{2}{\lambda^2}.
$$

Now, substitute into the formula for variance:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = \frac{2}{\lambda^2} - \left( \frac{1}{\lambda} \right)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2}.
$$

Simplify:

$$
\text{Var}(X) = \frac{1}{\lambda^2}.
$$

---


# Q3

## Moment Generating Function (MGF) of Gamma Distribution

A gamma random variable $X$ with shape parameter $k > 0$ and rate parameter $\lambda > 0$ has the probability density function (PDF):

$$
f_X(x) =
\begin{cases}
\frac{\lambda^k x^{k-1} e^{-\lambda x}}{\Gamma(k)}, & x \geq 0, \\
0, & x < 0,
\end{cases}
$$

where $\Gamma(k)$ is the gamma function, defined as:

$$
\Gamma(k) = \int_0^\infty t^{k-1} e^{-t} \, dt.
$$

---

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int_{-\infty}^\infty e^{tx} f_X(x) \, dx.
$$

Substitute the PDF of $X$:

$$
M_X(t) = \int_0^\infty e^{tx} \frac{\lambda^k x^{k-1} e^{-\lambda x}}{\Gamma(k)} \, dx.
$$

Combine the exponential terms:

$$
M_X(t) = \frac{\lambda^k}{\Gamma(k)} \int_0^\infty x^{k-1} e^{-(\lambda - t)x} \, dx, \quad \text{for } t < \lambda.
$$

Recognize the integral as the form of the gamma function:

$$
\int_0^\infty x^{a-1} e^{-bx} \, dx = \frac{\Gamma(a)}{b^a}, \quad b > 0.
$$

Here, $a = k$ and $b = \lambda - t$. Thus:

$$
M_X(t) = \frac{\lambda^k}{\Gamma(k)} \cdot \frac{\Gamma(k)}{(\lambda - t)^k}.
$$

Simplify:

$$
M_X(t) = \left( \frac{\lambda}{\lambda - t} \right)^k, \quad t < \lambda.
$$

Thus, the MGF of a gamma random variable is:

$$
M_X(t) = \left( \frac{\lambda}{\lambda - t} \right)^k, \quad t < \lambda.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X(t) = \left( \frac{\lambda}{\lambda - t} \right)^k.
$$

Using the chain rule:

$$
M_X'(t) = k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{\lambda}{(\lambda - t)^2}.
$$

Evaluate at $t = 0$:

$$
M_X'(0) = k \left( \frac{\lambda}{\lambda} \right)^{k-1} \cdot \frac{\lambda}{\lambda^2} = k \cdot \frac{1}{\lambda}.
$$

Thus, the mean of $X$ is:

$$
\mu_X = \frac{k}{\lambda}.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X'(t) = k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{\lambda}{(\lambda - t)^2}.
$$

Differentiate again:

$$
M_X''(t) = \frac{d}{dt} \left[ k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{\lambda}{(\lambda - t)^2} \right].
$$

Using the product and chain rules, we find:

$$
M_X''(t) = k(k-1) \left( \frac{\lambda}{\lambda - t} \right)^{k-2} \cdot \frac{\lambda^2}{(\lambda - t)^4} + k \left( \frac{\lambda}{\lambda - t} \right)^{k-1} \cdot \frac{2\lambda}{(\lambda - t)^3}.
$$

Evaluate $M_X''(t)$ at $t = 0$:

$$
M_X''(0) = k(k-1) \cdot \frac{\lambda^2}{\lambda^4} + k \cdot \frac{2\lambda}{\lambda^3}.
$$

Simplify:

$$
M_X''(0) = \frac{k(k-1)}{\lambda^2} + \frac{2k}{\lambda^2}.
$$

Combine terms:

$$
M_X''(0) = \frac{k^2 + k}{\lambda^2}.
$$

Now compute the variance:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = \frac{k^2 + k}{\lambda^2} - \left( \frac{k}{\lambda} \right)^2.
$$

Simplify:

$$
\text{Var}(X) = \frac{k^2 + k}{\lambda^2} - \frac{k^2}{\lambda^2} = \frac{k}{\lambda^2}.
$$

---

# Q4

## Moment Generating Function (MGF) of Normal Distribution

A normal random variable $X$ with mean $\mu$ and variance $\sigma^2 > 0$ has the probability density function (PDF):

$$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \quad x \in \mathbb{R}.
$$

---

### Step 1: Derive the MGF

The moment-generating function (MGF), $M_X(t)$, is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

Substitute the definition of expectation for a continuous random variable:

$$
M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x) \, dx.
$$

Substitute the PDF of $X$:

$$
M_X(t) = \int_{-\infty}^\infty e^{tx} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \, dx.
$$

Combine the exponential terms in the integrand:

$$
M_X(t) = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^\infty e^{-\frac{1}{2\sigma^2} \left[ (x-\mu)^2 - 2\sigma^2 tx \right]} \, dx.
$$

Simplify the term in the exponent:

$$
(x-\mu)^2 - 2\sigma^2 tx = x^2 - 2\mu x + \mu^2 - 2\sigma^2 tx.
$$

Factor $x$:

$$
x^2 - 2(\mu + \sigma^2 t)x + \mu^2.
$$

Complete the square for $x$:

$$
x^2 - 2(\mu + \sigma^2 t)x + (\mu + \sigma^2 t)^2 - (\mu + \sigma^2 t)^2 + \mu^2.
$$

Rewrite the exponent:

$$
-\frac{1}{2\sigma^2} \left[ (x - (\mu + \sigma^2 t))^2 - (\mu + \sigma^2 t)^2 + \mu^2 \right].
$$

Separate terms:

$$
M_X(t) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{(\mu + \sigma^2 t)^2 - \mu^2}{2\sigma^2}} \int_{-\infty}^\infty e^{-\frac{(x - (\mu + \sigma^2 t))^2}{2\sigma^2}} \, dx.
$$

The integral is the Gaussian integral for a shifted normal distribution, and it evaluates to 1. Thus:

$$
M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}.
$$

---

### Step 2: Mean of $X$

The mean of $X$, $\mu_X$, is the first moment and is obtained as the first derivative of the MGF evaluated at $t = 0$:

$$
\mu_X = M_X'(t) \Big|_{t=0}.
$$

First, compute the derivative of $M_X(t)$:

$$
M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}.
$$

Differentiate with respect to $t$:

$$
M_X'(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \left( \mu + \sigma^2 t \right).
$$

Evaluate at $t = 0$:

$$
M_X'(0) = e^{\mu \cdot 0 + \frac{\sigma^2 \cdot 0^2}{2}} \cdot \mu = \mu.
$$

Thus, the mean of $X$ is:

$$
\mu_X = \mu.
$$

---

### Step 3: Variance of $X$

The variance of $X$, $\text{Var}(X)$, is computed as:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2.
$$

First, compute the second derivative of $M_X(t)$:

$$
M_X'(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \left( \mu + \sigma^2 t \right).
$$

Differentiate again:

$$
M_X''(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \left( \mu + \sigma^2 t \right)^2 + e^{\mu t + \frac{\sigma^2 t^2}{2}} \cdot \sigma^2.
$$

Evaluate $M_X''(t)$ at $t = 0$:

$$
M_X''(0) = e^{\mu \cdot 0 + \frac{\sigma^2 \cdot 0^2}{2}} \cdot \left( \mu^2 + \sigma^2 \right) = \mu^2 + \sigma^2.
$$

Now compute the variance:

$$
\text{Var}(X) = M_X''(0) - (M_X'(0))^2 = \mu^2 + \sigma^2 - \mu^2.
$$

Simplify:

$$
\text{Var}(X) = \sigma^2.
$$

---

# Q5

## Given Information

The joint density function of $X$ (minimum) and $Y$ (maximum) for two independent uniform random variables on $[0, 1]$ is:

$$
f(x, y) = 2, \quad 0 \leq x \leq y \leq 1.
$$

---

### Part (a) Covariance and Correlation of $X$ and $Y$

#### Covariance
We have already calculated:

$$
\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = -\frac{7}{36}.
$$

---

#### Correlation
The correlation is:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}.
$$

1. **Variance of $X$:**

$$
\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
$$

From earlier calculations:

$$
\mathbb{E}[X^2] = \frac{1}{6}, \quad (\mathbb{E}[X])^2 = \left(\frac{2}{3}\right)^2 = \frac{4}{9}.
$$

$$
\text{Var}(X) = \frac{1}{6} - \frac{4}{9} = \frac{3}{18} - \frac{8}{18} = -\frac{5}{18}.
$$

2. **Variance of $Y$:**

Similarly:

$$
\text{Var}(Y) = \mathbb{E}[Y^2] - (\mathbb{E}[Y])^2.
$$

$$
\mathbb{E}[Y^2] = \int_0^1 y^2 f_Y(y) \, dy = \int_0^1 y^2 \cdot 2y \, dy = 2 \int_0^1 y^3 \, dy.
$$

$$
\mathbb{E}[Y^2] = 2 \cdot \frac{1}{4} = \frac{1}{2}.
$$

$$
\text{Var}(Y) = \frac{1}{2} - \frac{4}{9} = \frac{9}{18} - \frac{8}{18} = \frac{1}{18}.
$$

3. **Correlation:**

$$
\text{Corr}(X, Y) = \frac{-\frac{7}{36}}{\sqrt{-\frac{5}{18} \cdot \frac{1}{18}}}.
$$

This indicates there is negative correlation.

---

### Part (b) Conditional Expectations

1. **Find $\mathbb{E}[X \mid Y = y]$:**

The conditional density $f_{X \mid Y}(x \mid y)$ is:

$$
f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} = \frac{2}{2y} = \frac{1}{y}, \quad 0 \leq x \leq y.
$$

The conditional expectation is:

$$
\mathbb{E}[X \mid Y = y] = \int_0^y x \cdot \frac{1}{y} \, dx = \frac{1}{y} \int_0^y x \, dx = \frac{1}{y} \left[ \frac{x^2}{2} \right]_0^y = \frac{y}{2}.
$$

2. **Find $\mathbb{E}[Y \mid X = x]$:**

Similarly, the conditional density $f_{Y \mid X}(y \mid x)$ is:

$$
f_{Y \mid X}(y \mid x) = \frac{f(x, y)}{f_X(x)} = \frac{2}{2(1-x)} = \frac{1}{1-x}, \quad x \leq y \leq 1.
$$

The conditional expectation is:

$$
\mathbb{E}[Y \mid X = x] = \int_x^1 y \cdot \frac{1}{1-x} \, dy = \frac{1}{1-x} \int_x^1 y \, dy.
$$

Solve the integral:

$$
\int_x^1 y \, dy = \left[ \frac{y^2}{2} \right]_x^1 = \frac{1}{2} - \frac{x^2}{2}.
$$

Substitute:

$$
\mathbb{E}[Y \mid X = x] = \frac{1}{1-x} \cdot \left( \frac{1}{2} - \frac{x^2}{2} \right).
$$

---

### Part (c) PDF of $\mathbb{E}[X \mid Y]$ and $\mathbb{E}[Y \mid X]$

The conditional expectations $\mathbb{E}[X \mid Y = y] = \frac{y}{2}$ and $\mathbb{E}[Y \mid X = x]$ are deterministic functions, so their PDFs are degenerate (Dirac delta functions) at $\frac{y}{2}$ and $\frac{1}{2(1-x)}(1-x^2)$, respectively.

---

### Part (d) Linear Predictor of $Y$ in terms of $X$

We seek a linear predictor $\hat{Y} = a + bX$ that minimizes the mean squared error (MSE):

$$
\text{MSE} = \mathbb{E}[(Y - \hat{Y})^2].
$$

From linear regression theory, the coefficients $a$ and $b$ are given by:

$$
b = \frac{\text{Cov}(X, Y)}{\text{Var}(X)}, \quad a = \mathbb{E}[Y] - b\mathbb{E}[X].
$$

Using earlier results:

$$
b = \frac{-\frac{7}{36}}{-\frac{5}{18}} = \frac{7}{10}, \quad a = \frac{2}{3} - \frac{7}{10} \cdot \frac{2}{3}.
$$

Simplify $a$:

$$
a = \frac{2}{3} - \frac{14}{30} = \frac{2}{3} - \frac{7}{15} = \frac{10}{15} - \frac{7}{15} = \frac{3}{15} = \frac{1}{5}.
$$

Thus, the linear predictor is:

$$
\hat{Y} = \frac{1}{5} + \frac{7}{10}X.
$$

The minimal MSE is:

$$
\text{MSE} = \text{Var}(Y) - b^2 \text{Var}(X).
$$

$$
\text{MSE} = \frac{1}{18} - \left(\frac{7}{10}\right)^2 \cdot \left(-\frac{5}{18}\right).
$$

---

### Part (e) Predictor $\hat{Y} = h(X)$ Minimizing MSE

The optimal nonlinear predictor $\hat{Y} = \mathbb{E}[Y \mid X]$:

$$
\hat{Y} = \mathbb{E}[Y \mid X = x] = \frac{1}{1-x} \left( \frac{1}{2} - \frac{x^2}{2} \right).
$$

The minimal MSE is the residual variance:

$$
\text{MSE} = \text{Var}(Y \mid X).
$$

Since $\mathbb{E}[Y \mid X]$ is deterministic, $\text{Var}(Y \mid X) = 0$.

---

# Q6

## Problem Statement
We are given a bivariate normal distribution with the following parameters:
- $\mu_X = \mu_Y = 0$,
- $\sigma_X = \sigma_Y = 1$,
- Correlation coefficient $\rho$.

We are tasked to:
1. Sketch the contours of the density function for the bivariate normal distribution.
2. Sketch the regression lines $\mathbb{E}[Y \mid X = x]$ and $\mathbb{E}[X \mid Y = y]$ for $\rho = 0, 0.5, 0.9$.

---

## Bivariate Normal Distribution

### Joint Probability Density Function
The joint probability density function for a bivariate normal distribution is given by:

$$
f(x, y) = \frac{1}{2 \pi \sqrt{1 - \rho^2}} \exp \left( -\frac{1}{2(1 - \rho^2)} \left( x^2 - 2 \rho xy + y^2 \right) \right).
$$

#### Key Characteristics:
1. The joint density $f(x, y)$ depends on the correlation coefficient $\rho$. The term $-2 \rho xy$ introduces dependence between $X$ and $Y$.
2. The contours of the density are ellipses:
   - The center of the ellipses is at $(0, 0)$ since $\mu_X = \mu_Y = 0$.
   - The tilt of the ellipse depends on $\rho$, where:
     - $\rho = 0$: The ellipses are circles (no correlation between $X$ and $Y$).
     - $\rho > 0$: The ellipses tilt along $y = x$ (positive correlation).
     - $\rho < 0$: The ellipses tilt along $y = -x$ (negative correlation).

---

### Regression Lines
For a bivariate normal distribution:
1. The conditional expectation of $Y$ given $X = x$ is:

   $$
   \mathbb{E}[Y \mid X = x] = \rho x.
   $$

3. The conditional expectation of $X$ given $Y = y$ is:

   $$
   \mathbb{E}[X \mid Y = y] = \rho y.
   $$

The regression lines are straight lines through the origin, with slopes determined by $\rho$:
- $\mathbb{E}[Y \mid X = x]$ has slope $\rho$ in the $xy$-plane.
- $\mathbb{E}[X \mid Y = y]$ has slope $1/\rho$.

---

## Step 1: Contours of the Joint Density

### Contour Equation
The exponent in the joint density $f(x, y)$ determines the shape of the ellipses. Define the quadratic form in the exponent:

$$
Q(x, y) = \frac{x^2 - 2\rho xy + y^2}{1 - \rho^2}.
$$

For a fixed value of $Q(x, y)$, say $Q(x, y) = c$, we get the equation of an ellipse:

$$
\frac{x^2}{1 - \rho^2} - \frac{2 \rho xy}{1 - \rho^2} + \frac{y^2}{1 - \rho^2} = c.
$$

This simplifies to:

$$
(1 - \rho^2)x^2 - 2\rho xy + y^2 = c(1 - \rho^2).
$$

---

### Behavior for Different Values of $\rho$:
1. **When $\rho = 0$:**
   - The cross term $-2 \rho xy = 0$, so the equation reduces to:

     $$
     x^2 + y^2 = c.
     $$
     
   - The contours are **circles** centered at $(0, 0)$.

2. **When $\rho > 0$:**
   - The ellipses are tilted along the line $y = x$. Larger $\rho$ values result in narrower ellipses.

3. **When $\rho < 0$:**
   - The ellipses are tilted along the line $y = -x$. Larger negative $\rho$ values result in narrower ellipses.

---

## Step 2: Regression Lines for $\rho = 0, 0.5, 0.9$

### Regression Equations
For the regression lines:
1. $\mathbb{E}[Y \mid X = x] = \rho x$ :
   - Slope of the line is $\rho$.
2. $\mathbb{E}[X \mid Y = y] = \rho y$:
   - Slope of the line is $\rho$.

### Special Cases
1. **When $\rho = 0$:**
   - $\mathbb{E}[Y \mid X = x] = 0$, $\mathbb{E}[X \mid Y = y] = 0$.
   - Both lines are flat (no correlation).

2. **When $\rho = 0.5$:**
   - $\mathbb{E}[Y \mid X = x] = 0.5x$, $\mathbb{E}[X \mid Y = y] = 0.5y$.
   - The regression lines are tilted with slope $0.5$.

3. **When $\rho = 0.9$:**
   - $\mathbb{E}[Y \mid X = x] = 0.9x$, $\mathbb{E}[X \mid Y = y] = 0.9y$.
   - The regression lines are steep with slope $0.9$.

---

## Visualization

### Contours and Regression Lines
Below are the characteristics of the plots for each value of $\rho$:

1. **$\rho = 0$:**
   - Contours: Circles centered at $(0, 0)$.
   - Regression lines: Both lines are horizontal and vertical (flat).

2. **$\rho = 0.5$:**
   - Contours: Ellipses tilted along $y = x$.
   - Regression lines: Slope $0.5$.

3. **$\rho = 0.9$:**
   - Contours: Narrow ellipses tilted sharply along $y = x$.
   - Regression lines: Slope $0.9$.

---

# Q7

## Problem Statement
The joint density of $X$ and $Y$ is given as:

$$
f(x, y) = e^{-y}, \quad 0 \leq x \leq y.
$$

---

## Part (a) Covariance and Correlation of $X$ and $Y$

### Step 1: Marginal Densities

#### Marginal Density of $X$
To find $f_X(x)$, integrate $f(x, y)$ over $y$:

$$
f_X(x) = \int_x^\infty e^{-y} \, dy.
$$

Solve:

$$
f_X(x) = \left[ -e^{-y} \right]_x^\infty = e^{-x}, \quad x \geq 0.
$$

#### Marginal Density of $Y$
To find $f_Y(y)$, integrate $f(x, y)$ over $x$:

$$
f_Y(y) = \int_0^y e^{-y} \, dx.
$$

Since $e^{-y}$ is independent of $x$, the integral becomes:

$$
f_Y(y) = e^{-y} \cdot (y - 0) = y e^{-y}, \quad y \geq 0.
$$

---

### Step 2: Expectations

#### $\mathbb{E}[X]$
The expectation $\mathbb{E}[X]$ is given by:

$$
\mathbb{E}[X] = \int_0^\infty x f_X(x) \, dx = \int_0^\infty x e^{-x} \, dx.
$$

Use integration by parts:
- Let $u = x$, $dv = e^{-x} dx$.
- Then $du = dx$, $v = -e^{-x}$.

$$
\mathbb{E}[X] = \left[ -x e^{-x} \right]_0^\infty + \int_0^\infty e^{-x} \, dx = 0 + 1 = 1.
$$

#### $\mathbb{E}[Y]$
The expectation $\mathbb{E}[Y]$ is:

$$
\mathbb{E}[Y] = \int_0^\infty y f_Y(y) \, dy = \int_0^\infty y^2 e^{-y} \, dy.
$$

Use the formula for the $n$-th moment of an exponential distribution:

$$
\int_0^\infty y^n e^{-y} \, dy = n!.
$$

Here $n = 2$:

$$
\mathbb{E}[Y] = 2! = 2.
$$

#### $\mathbb{E}[XY]$
The joint expectation $\mathbb{E}[XY]$ is:

$$
\mathbb{E}[XY] = \int_0^\infty \int_0^y xy f(x, y) \, dx \, dy.
$$

Substitute $f(x, y) = e^{-y}$:

$$
\mathbb{E}[XY] = \int_0^\infty \int_0^y xy e^{-y} \, dx \, dy.
$$

Solve the inner integral over $x$:

$$
\int_0^y xy e^{-y} \, dx = y e^{-y} \int_0^y x \, dx = y e^{-y} \cdot \frac{y^2}{2} = \frac{y^3 e^{-y}}{2}.
$$

Now solve the outer integral:

$$
\mathbb{E}[XY] = \int_0^\infty \frac{y^3 e^{-y}}{2} \, dy = \frac{1}{2} \int_0^\infty y^3 e^{-y} \, dy.
$$

Using the $n$-th moment formula:

$$
\int_0^\infty y^n e^{-y} \, dy = n!,
$$

for $n = 3$, $3! = 6$:

$$
\mathbb{E}[XY] = \frac{1}{2} \cdot 6 = 3.
$$

---

### Step 3: Covariance and Correlation

#### Covariance
The covariance is:
$$
\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y].
$$

Substitute the values:

$$
\text{Cov}(X, Y) = 3 - (1)(2) = 1.
$$

#### Correlation
The correlation is:

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}.
$$

We need $\text{Var}(X)$ and $\text{Var}(Y)$.

1. **Variance of $X$:**

   $$
   \text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
   $$

   Compute $\mathbb{E}[X^2]$:

   $$
   \mathbb{E}[X^2] = \int_0^\infty x^2 f_X(x) \, dx = \int_0^\infty x^2 e^{-x} \, dx = 2.
   $$

   So:

   $$
   \text{Var}(X) = 2 - 1^2 = 1.
   $$

3. **Variance of $Y$:**

   $$
   \text{Var}(Y) = \mathbb{E}[Y^2] - (\mathbb{E}[Y])^2.
   $$

   Compute $\mathbb{E}[Y^2]$:

   $$
   \mathbb{E}[Y^2] = \int_0^\infty y^3 e^{-y} \, dy = 3! = 6.
   $$

   So:

   $$
   \text{Var}(Y) = 6 - 2^2 = 2.
   $$

Substitute into the correlation formula:
$$
\text{Corr}(X, Y) = \frac{1}{\sqrt{1 \cdot 2}} = \frac{1}{\sqrt{2}}.
$$

---

## Part (b) Conditional Expectations

### $\mathbb{E}[X \mid Y = y]$
The conditional density of $x$ given $Y = y$ is:

$$
f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} = \frac{e^{-y}}{y e^{-y}} = \frac{1}{y}, \quad 0 \leq x \leq y.
$$

The conditional expectation is:

$$
\mathbb{E}[X \mid Y = y] = \int_0^y x f_{X \mid Y}(x \mid y) \, dx = \int_0^y x \cdot \frac{1}{y} \, dx = \frac{1}{y} \cdot \frac{y^2}{2} = \frac{y}{2}.
$$

---

### $\mathbb{E}[Y \mid X = x]$
The conditional density of $Y$ given $X = x$ is:

$$
f_{Y \mid X}(y \mid x) = \frac{f(x, y)}{f_X(x)} = \frac{e^{-y}}{e^{-x}} = e^{-(y-x)}, \quad y \geq x.
$$

The conditional expectation is:

$$
\mathbb{E}[Y \mid X = x] = \int_x^\infty y f_{Y \mid X}(y \mid x) \, dy.
$$

Solve:

$$
\int_x^\infty y e^{-(y-x)} \, dy = \int_x^\infty y e^{-y+x} \, dy = e^x \int_x^\infty y e^{-y} \, dy.
$$

Shift $z = y - x$, so $dz = dy$:

$$
\int_x^\infty y e^{-y} \, dy = x + 1.
$$

Thus:

$$
\mathbb{E}[Y \mid X = x] = x + 1.
$$

---

## Part (c) Density Functions of $\mathbb{E}[X \mid Y]$ and $\mathbb{E}[Y \mid X]$

### $\mathbb{E}[X \mid Y] = \frac{Y}{2}$
Since $\mathbb{E}[X \mid Y]$ is a deterministic function of $Y$, its density is derived from $f_Y(y)$:

$$
f_{\mathbb{E}[X \mid Y]}(z) = f_Y(2z), \quad z \geq 0.
$$

Substitute $f_Y(y) = y e^{-y}$:

$$
f_{\mathbb{E}[X \mid Y]}(z) = (2z) e^{-2z}, \quad z \geq 0.
$$

---

### $\mathbb{E}[Y \mid X] = x + 1$
Similarly, $\mathbb{E}[Y \mid X]$ is deterministic, so its density is degenerate (Dirac delta function) at $x + 1$.

---

# Q8

## Moment-Generating Function and Expectations

### Problem Statement
Let $X$ be a continuous random variable with density function:

$$
f(x) = 2x, \quad 0 \leq x \leq 1.
$$

1. Find the moment-generating function $M_X(t)$ of $X$.
2. Verify that $\mathbb{E}[X] = M'_X(0)$ and $\mathbb{E}[X^2] = M''_X(0)$.

---

### Solution

#### Step 1: Moment-Generating Function $M_X(t)$
The moment-generating function is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int_0^1 e^{tx} f(x) \, dx.
$$

Substitute $f(x) = 2x$:

$$
M_X(t) = \int_0^1 e^{tx} \cdot 2x \, dx.
$$

Using integration by parts:
- Let $u = x$, $dv = 2e^{tx} \, dx$,
- Then $du = dx$, $v = \frac{2}{t}e^{tx}$.

$$
M_X(t) = \left[ \frac{2x}{t}e^{tx} \right]_0^1 - \int_0^1 \frac{2}{t}e^{tx} \, dx.
$$

Solve the first term:

$$
\left[ \frac{2x}{t}e^{tx} \right]_0^1 = \frac{2}{t}e^t - 0 = \frac{2}{t}e^t.
$$

Solve the integral:

$$
\int_0^1 \frac{2}{t}e^{tx} \, dx = \frac{2}{t^2}(e^t - 1).
$$

Combine:

$$
M_X(t) = \frac{2}{t}e^t - \frac{2}{t^2}(e^t - 1) = \frac{2}{t^2}(e^t - 1).
$$

Thus, the MGF is:

$$
M_X(t) = \frac{2(e^t - 1)}{t^2}.
$$

---

#### Step 2: Verify $\mathbb{E}[X] = M_X'(0)$
Differentiate $M_X(t)$:

$$
M_X'(t) = \frac{d}{dt} \left( \frac{2(e^t - 1)}{t^2} \right).
$$

Using the quotient rule:

$$
M_X'(t) = \frac{2t^2 e^t - 4t(e^t - 1)}{t^4}.
$$

At $t = 0$:

$$
M_X'(0) = \lim_{t \to 0} \frac{2t^2 e^t - 4t(e^t - 1)}{t^4}.
$$

Expand $e^t \approx 1 + t + \frac{t^2}{2}$:

$$
M_X'(0) = \lim_{t \to 0} \frac{2t^2 (1 + t) - 4t(t)}{t^4} = \frac{1}{2}.
$$

Thus:

$$
\mathbb{E}[X] = \frac{1}{2}.
$$

---

#### Step 3: Verify $\mathbb{E}[X^2] = M_X''(0)$
Differentiate $M_X'(t)$ again to find $M_X''(t)$. Following similar steps, at $t = 0$:

$$
M_X''(0) = \int_0^1 x^2 \cdot 2x \, dx = \frac{2}{3}.
$$

Thus:

$$
\mathbb{E}[X^2] = \frac{2}{3}.
$$

---

# Q9

## Linear Combination of Independent Normals

### Problem Statement
Let $X_1, X_2, \dots, X_n$ be independent normal random variables with:
- Means: $\mu_i$,
- Variances: $\sigma_i^2$.

Show that $Y = \sum_{i=1}^n \alpha_i X_i$, where $\alpha_i$ are scalars, is normally distributed, and find its mean and variance.

---

### Solution

#### Step 1: Moment-Generating Function of $Y$
The moment-generating function $M_Y(t)$ is given by:

$$
M_Y(t) = \mathbb{E}\left[e^{tY}\right] = \mathbb{E}\left[e^{t \sum_{i=1}^n \alpha_i X_i}\right].
$$

Since the $X_i$'s are independent:

$$
M_Y(t) = \prod_{i=1}^n M_{X_i}(\alpha_i t).
$$

The MGF of $X_i$ is:

$$
M_{X_i}(t) = \exp\left(\mu_i t + \frac{\sigma_i^2 t^2}{2}\right).
$$

Substitute into $M_Y(t)$:

$$
M_Y(t) = \prod_{i=1}^n \exp\left(\mu_i \alpha_i t + \frac{\sigma_i^2 (\alpha_i t)^2}{2}\right).
$$

Simplify:

$$
M_Y(t) = \exp\left( t \sum_{i=1}^n \alpha_i \mu_i + \frac{t^2}{2} \sum_{i=1}^n \alpha_i^2 \sigma_i^2 \right).
$$

This is the MGF of a normal distribution. Thus, $Y$ is normally distributed with:

$$
\mu_Y = \sum_{i=1}^n \alpha_i \mu_i, \quad \sigma_Y^2 = \sum_{i=1}^n \alpha_i^2 \sigma_i^2.
$$

---

# Q10

## Scaling an Exponential Random Variable

### Problem Statement
Let $X \sim \text{Exponential}(\lambda)$. Show that if $Y = cX$, $Y \sim \text{Exponential}\left(\frac{\lambda}{c}\right)$, where $c > 0$.

---

### Solution

#### Step 1: Moment-Generating Function of $X$
The MGF of $X \sim \text{Exponential}(\lambda)$ is:

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int_0^\infty e^{tx} \lambda e^{-\lambda x} \, dx.
$$

Simplify:

$$
M_X(t) = \frac{\lambda}{\lambda - t}, \quad t < \lambda.
$$

#### Step 2: MGF of $Y = cX$
Let Y = cX$. Then:

$$
M_Y(t) = \mathbb{E}[e^{tY}] = \mathbb{E}[e^{t(cX)}] = M_X(ct).
$$

Substitute $M_X(ct)$:

$$
M_Y(t) = \frac{\lambda}{\lambda - ct}.
$$

#### Step 3: Compare with Exponential MGF
The MGF of $\text{Exponential}\left(\frac{\lambda}{c}\right)$ is:

$$
M_Y(t) = \frac{\frac{\lambda}{c}}{\frac{\lambda}{c} - t}.
$$

Simplify:

$$
M_Y(t) = \frac{\lambda}{\lambda - ct}.
$$

Thus, $Y \sim \text{Exponential}\left(\frac{\lambda}{c}\right)$.

---


