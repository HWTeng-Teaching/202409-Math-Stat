Lina

## Q1: Mean and Variance of a Bernoulli Random Variable
A Bernoulli random variable $X$ takes value 1 with probability $p$ and 0 with probability $1-p$.

- **Mean**:
  $E(X) = 1 \cdot p + 0 \cdot (1-p) = p$

- **Variance**:
  $\text{Var}(X) = E(X^2) - [E(X)]^2$

  Since $X^2 = X$ for a Bernoulli random variable:
  $E(X^2) = E(X) = p$
  
  $\text{Var}(X) = p - p^2 = p(1-p)$

---

## Q2: Mean and Variance of an Exponential Random Variable
For an exponential random variable $X$ with rate parameter $\lambda > 0$:

- **Mean**:
  $E(X) = \frac{1}{\lambda}$

- **Variance**:
  $\text{Var}(X) = \frac{1}{\lambda^2}$

---

## Q3: Mean and Variance of a Gamma Random Variable
For a Gamma random variable $X$ with shape parameter $k$ and rate parameter $\lambda$:

- **Mean**:
  $E(X) = \frac{k}{\lambda}$

- **Variance**:
  $\text{Var}(X) = \frac{k}{\lambda^2}$

---

## Q4: Mean and Variance of a Normal Random Variable
For a Normal random variable $X \sim N(\mu, \sigma^2)$:

- **Mean**:
  $E(X) = \mu$

- **Variance**:
  $\text{Var}(X) = \sigma^2$

---

## Q5: Problem Based on Provided Image
### Given:
The random variable $X$ has a probability density function (PDF):
$f(x) = 2x, \quad 0 \leq x \leq 1.$

#### (a) Find $E(X)$:
The expectation of $X$ is given by:
$E(X) = \int_{0}^{1} x f(x) \, dx = \int_{0}^{1} x (2x) \, dx = \int_{0}^{1} 2x^2 \, dx.$

Compute the integral:
$E(X) = 2 \int_{0}^{1} x^2 \, dx = 2 \left[\frac{x^3}{3}\right]_{0}^{1} = 2 \cdot \frac{1}{3} = \frac{2}{3}.$

#### (b) Let $Y = X^2$. Find the probability mass function (PMF) of $Y$ and use it to find $E(Y)$:
If $Y = X^2$, then the PDF of $Y$ can be derived using a transformation. However, we calculate $E(Y)$ directly:
$E(Y) = E(X^2) = \int_{0}^{1} x^2 f(x) \, dx = \int_{0}^{1} x^2 (2x) \, dx = \int_{0}^{1} 2x^3 \, dx.$
Compute the integral:
$E(Y) = 2 \int_{0}^{1} x^3 \, dx = 2 \left[\frac{x^4}{4}\right]_{0}^{1} = 2 \cdot \frac{1}{4} = \frac{1}{2}.$

#### (c) Use Theorem A in Section 4.1.1 to find $E(X^2)$ and compare to (b):
From part (b), $E(X^2) = \frac{1}{2}$. This matches the result found for $E(Y)$ in part (b).

#### (d) Find $\text{Var}(X)$ using the definition of variance and Theorem B of Section 4.2:
The variance of $X$ is:
$\text{Var}(X) = E(X^2) - [E(X)]^2.$
Substitute $E(X^2) = \frac{1}{2}$ and $E(X) = \frac{2}{3}$:
$\text{Var}(X) = \frac{1}{2} - \left(\frac{2}{3}\right)^2 = \frac{1}{2} - \frac{4}{9}.$
Simplify:
$\text{Var}(X) = \frac{9}{18} - \frac{8}{18} = \frac{1}{18}.$

## Q6: Standardizing a Random Variable
### Problem:
Let $E(X) = \mu$ and $\text{Var}(X) = \sigma^2$. Define $Z = \frac{X - \mu}{\sigma}$. Show that $E(Z) = 0$ and $\text{Var}(Z) = 1$.

### Solution:
1. **Expected Value of $Z$**:
   $E(Z) = E\left(\frac{X - \mu}{\sigma}\right) = \frac{1}{\sigma} E(X - \mu).$
   Since $E(X - \mu) = E(X) - \mu = \mu - \mu = 0$:
   $E(Z) = 0.$

2. **Variance of $Z$**:
   $\text{Var}(Z) = \text{Var}\left(\frac{X - \mu}{\sigma}\right) = \frac{1}{\sigma^2} \text{Var}(X - \mu).$
   Since $\text{Var}(X - \mu) = \text{Var}(X) = \sigma^2$:
   $\text{Var}(Z) = \frac{\sigma^2}{\sigma^2} = 1.$

---

## Q7: Expectation of $1/X$ for a Uniform Random Variable
### Problem:
Let $X$ be uniformly distributed on $[1, 2]$. Find $E(1/X)$. Is $E(1/X) = 1/E(X)$?

### Solution:
1. **Find $E(1/X)$**:
   The PDF of $X$ is $f(x) = 1$ for $x \in [1, 2]$. Then:
   $E\left(\frac{1}{X}\right) = \int_{1}^{2} \frac{1}{x} f(x) dx = \int_{1}^{2} \frac{1}{x} dx.$
   Compute the integral:
   $E\left(\frac{1}{X}\right) = \left[\ln(x)\right]_{1}^{2} = \ln(2) - \ln(1) = \ln(2).$

2. **Compare $E(1/X)$ with $1/E(X)$**:
   First, find $E(X)$:
   $E(X) = \int_{1}^{2} x f(x) dx = \int_{1}^{2} x dx = \left[\frac{x^2}{2}\right]_{1}^{2} = \frac{4}{2} - \frac{1}{2} = \frac{3}{2}.$
   Then:
   $1/E(X) = \frac{1}{3/2} = \frac{2}{3}.$
   
   Clearly, $E(1/X) = \ln(2) \neq 1/E(X)$.

---

## Q8: Chebyshev’s Inequality for an Exponential Random Variable
### Problem:
Let $X$ be an exponential random variable with standard deviation $\sigma$. Find $P(|X - E(X)| > k\sigma)$ for $k = 2, 3, 4$ and compare the results to Chebyshev’s inequality.

### Solution:
1. **Exponential Distribution Properties**:
   For an exponential random variable with rate parameter $\lambda$, we have:
   $E(X) = \frac{1}{\lambda}, \quad \text{Var}(X) = \frac{1}{\lambda^2}, \quad \sigma = \frac{1}{\lambda}.$

2. **Probability Calculation**:
   For $k = 2$, $3$, $4$, calculate:
   $P(|X - E(X)| > k\sigma) = P\left(X > E(X) + k\sigma \right) + P\left(X < E(X) - k\sigma \right).$
   The exponential distribution is one-sided, so $P(X < E(X) - k\sigma) = 0$:
   $P(|X - E(X)| > k\sigma) = P\left(X > \frac{1}{\lambda} + k\frac{1}{\lambda}\right) = P\left(X > \frac{1 + k}{\lambda}\right).$
   Using the survival function of the exponential:
   $P(X > x) = e^{-\lambda x}.$
   Substitute $x = \frac{1 + k}{\lambda}$:
   $P(|X - E(X)| > k\sigma) = e^{-(1+k)}.$

3. **Chebyshev’s Inequality**:
   Chebyshev's bound is:
   $P(|X - E(X)| > k\sigma) \leq \frac{1}{k^2}.$

   Compare results:
   - For $k = 2$: $P(|X - E(X)| > 2\sigma) = e^{-3}$ vs. $\frac{1}{4}$.
   - For $k = 3$: $P(|X - E(X)| > 3\sigma) = e^{-4}$ vs. $\frac{1}{9}$.
   - For $k = 4$: $P(|X - E(X)| > 4\sigma) = e^{-5}$ vs. $\frac{1}{16}$.

---

## Q9: Variance of the Difference of Two Random Variables
### Problem:
Show that $\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X, Y)$.

## Solution:

### Step 1: Recall the definition of variance
The variance of a random variable $Z$ is defined as:

$\text{Var}(Z) = E[Z^2] - (E[Z])^2.$

Here, $Z = X - Y$, so we can write:

$\text{Var}(X - Y) = E[(X - Y)^2] - [E(X - Y)]^2.$

---

### Step 2: Expand $(X - Y)^2$
Expand the square term:

$(X - Y)^2 = X^2 - 2XY + Y^2.$

Take the expectation:

$E[(X - Y)^2] = E(X^2) - 2E(XY) + E(Y^2).$

Now, rewrite the variance:

$\text{Var}(X - Y) = \big[E(X^2) - 2E(XY) + E(Y^2)\big] - \big[E(X - Y)\big]^2.$

---

### Step 3: Break down $E(X - Y)$
Using the linearity of expectation:

$E(X - Y) = E(X) - E(Y).$

So:

$[E(X - Y)]^2 = [E(X) - E(Y)]^2.$

Substitute this into the variance equation:

$\text{Var}(X - Y) = \big[E(X^2) - 2E(XY) + E(Y^2)\big] - [E(X) - E(Y)]^2.$

---

### Step 4: Relate to variance and covariance
Recall the definitions of variance and covariance:
1. Variance:

   $\text{Var}(X) = E(X^2) - [E(X)]^2, \quad \text{Var}(Y) = E(Y^2) - [E(Y)]^2.$

3. Covariance:

   $\text{Cov}(X, Y) = E(XY) - E(X)E(Y).$

Using these definitions:
- $E(X^2) = \text{Var}(X) + [E(X)]^2$,
- $E(Y^2) = \text{Var}(Y) + [E(Y)]^2$,
- $E(XY) = \text{Cov}(X, Y) + E(X)E(Y)$.

---

### Step 5: Substitute into the variance equation
Substitute $E(X^2)$, $E(Y^2)$, and $E(XY)$ into:

$\text{Var}(X - Y) = \big[E(X^2) - 2E(XY) + E(Y^2)\big] - [E(X) - E(Y)]^2.$

1. Substitute $E(X^2)$ and $E(Y^2)$:

   $\text{Var}(X - Y) = \big[\text{Var}(X) + [E(X)]^2 - 2(E(XY)) + \text{Var}(Y) + [E(Y)]^2\big] - [E(X) - E(Y)]^2.$

3. Substitute $E(XY)$:

   $\text{Var}(X - Y) = \text{Var}(X) + [E(X)]^2 - 2[\text{Cov}(X, Y) + E(X)E(Y)] + \text{Var}(Y) + [E(Y)]^2 - [E(X) - E(Y)]^2.$

5. Simplify $[E(X) - E(Y)]^2$:

   $[E(X) - E(Y)]^2 = [E(X)]^2 - 2E(X)E(Y) + [E(Y)]^2.$

7. Subtract $[E(X) - E(Y)]^2$ from the rest:

   $\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X, Y).$

---

## Final Answer:
We have shown that:

$\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X, Y).$

## Q10: Covariance of $X+Y$ and $X-Y$
### Problem:
If $X$ and $Y$ are independent random variables with equal variances, find $\text{Cov}(X+Y, X-Y)$.

### Solution:
1. Expand the covariance:
   $\text{Cov}(X+Y, X-Y) = \text{Cov}(X, X) - \text{Cov}(X, Y) + \text{Cov}(Y, X) - \text{Cov}(Y, Y).$
2. Use independence ($\text{Cov}(X, Y) = 0$) and properties of covariance:
   $\text{Cov}(X+Y, X-Y) = \text{Var}(X) - \text{Var}(Y).$
3. Since $X$ and $Y$ have equal variances:
   $\text{Cov}(X+Y, X-Y) = 0.$

