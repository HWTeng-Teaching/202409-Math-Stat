# Q1
What are the key points in Lecture 1105?

In Lecture 1105, the primary focus was on **functions of jointly distributed random variables**, specifically within the context of **Chapter 3.6: Joint Distributions**. Here are the main topics discussed:

- **Transformation of Variables**: The lecture introduced transformations for jointly continuous random variables, providing tools to change variables and compute new joint distributions. This is crucial when moving from one coordinate system to another (e.g., Cartesian to polar coordinates) in statistical analysis.
  
- **Jacobian Determinant**: The Jacobian matrix and its determinant were used to facilitate transformations of random variables. The determinant of the Jacobian accounts for how areas (or volumes, in higher dimensions) scale under the transformation, ensuring that probabilities remain consistent.

- **Application to Joint Densities**: By using the Jacobian determinant, the lecture demonstrated how to find joint densities in transformed coordinates. For instance, converting jointly normal random variables \(X\) and \(Y\) to polar coordinates resulted in a different joint density function in terms of \(R\) (radius) and \(\theta\) (angle).

- **Examples**: The lecture presented examples that applied these transformations, showing how to calculate joint densities for transformed variables, which is useful for deriving new distributions or simplifying complex integrals in probability.

These key points provide the foundation for understanding transformations and applications in the context of joint distributions.

# Q2
Explain what is the Leibniz integration rule

The **Leibniz rule** is a method for differentiating an integral with respect to a parameter when the limits of integration also depend on the parameter. 
It is commonly used when differentiating functions defined by integrals, especially in probability and statistics.

For an integral of the form:

$$
\int_{a(z)}^{b(z)} f(z, u) \, du,
$$

where $z$ is a parameter, the Leibniz rule states:

$$
\frac{\partial}{\partial z} \left( \int_{a(z)}^{b(z)} f(z, u) \, du \right) = f(z, b(z)) \frac{d}{dz} b(z) - f(z, a(z)) \frac{d}{dz} a(z) + \int_{a(z)}^{b(z)} \frac{\partial}{\partial z} f(z, u) \, du.
$$

### Explanation of Each Term
1. **Boundary Terms** $f(z, b(z)) \frac{d}{dz} b(z) - f(z, a(z)) \frac{d}{dz} a(z)$:
   These account for the fact that the limits of integration, $a(z)$ and $b(z)$, vary with $z$.
3. **Integral of Partial Derivative** $\int_{a(z)}^{b(z)} \frac{\partial}{\partial z} f(z, u) \, du$:
   This term captures the rate of change of the integrand with respect to $z$ over the range of integration.

### Applications
The Leibniz rule is particularly useful in probability and calculus when dealing with cumulative distribution functions (CDFs), 
expected values, and other quantities where integrals depend on parameters.

# Q3
Identify the bivariate normal distribution on page 14 of the slides in class

The bivariate normal distribution identified on page 14 is characterized by the following joint density function:

$$
f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2 \pi \sigma_{Y_1} \sigma_{Y_2} \sqrt{1 - \rho^2}} \exp \left( -\frac{1}{2(1 - \rho^2)} \left( \frac{(y_1 - \mu_{Y_1})^2}{\sigma_{Y_1}^2} + \frac{(y_2 - \mu_{Y_2})^2}{\sigma_{Y_2}^2} - \frac{2 \rho (y_1 - \mu_{Y_1})(y_2 - \mu_{Y_2})}{\sigma_{Y_1} \sigma_{Y_2}} \right) \right).
$$

In this particular example:
- **Means**: $\mu_{Y_1} = 0$ and $\mu_{Y_2} = 0$,
- **Standard Deviations**: $\sigma_{Y_1} = 1$ and $\sigma_{Y_2} = 2$,
- **Correlation**: $\rho = \frac{1}{2}$.

Thus, the distribution is:

$$
(Y_1, Y_2) \sim N_2 \left( \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & 2 \end{pmatrix} \right),
$$

where $N_2$ denotes the bivariate normal distribution.

# Q4
Inverse Transform Method
The **inverse transform method** leverages the cumulative distribution function (CDF) of the desired distribution. 
By applying the inverse CDF (quantile function) of the standard normal distribution to uniformly distributed random variables, 
we obtain samples that follow the standard normal distribution.

### Steps and Code
1. **Generate Uniform Samples**: We start by drawing 1000 samples from the uniform distribution $U(0, 1)$.
2. **Apply the Inverse CDF**: For each sample, we apply the inverse CDF (also called the percent-point function) of the standard normal distribution.
   This transforms uniform samples into standard normal samples.

   ![image](https://github.com/user-attachments/assets/96dbd63e-2eb8-4092-b35b-abad2fba2d30)

# Q5
The **polar method**, or **Box-Muller transform**, is another way to generate pairs of independent standard normal samples. 
This method uses pairs of uniform random variables to produce normal samples through trigonometric transformations.

### Steps and Code

1. **Generate Pairs of Uniform Variables**: Draw two independent uniform random variables, $U_1$ and $U_2$.

2. **Calculate Radius and Angle**:
   - Compute $R = \sqrt{-2 \ln(U_1)}$.
   - Compute $\theta = 2 \pi U_2$.

3. **Calculate Normal Samples**:
   - Set $X = R \cos(\theta)$ and $Y = R \sin(\theta)$.
   - These values will each follow a standard normal distribution.

4. **Repeat**: Continue until we have 1000 total samples.

   # Why Use $U_1$ in the Box-Muller Transformation?

In the Box-Muller transformation, $U_1$, a uniform random variable on the interval $(0, 1)$, is essential for generating a **Rayleigh-distributed radius** $R$ 
that allows us to produce independent standard normal samples. 

### 1. Transforming Uniform to Rayleigh
The Box-Muller approach transforms a uniform random variable into one that follows the Rayleigh distribution, which is crucial for generating standard normal samples in 2D space. 
The Rayleigh distribution, with parameter $\sigma = 1$, has the probability density function:

$f_R(r) = r e^{-r^2 / 2}.$

This distribution describes the radial distance when points are scattered in 2D space, with each component following a normal distribution.

### 2. Inverse Transform Sampling
To generate a Rayleigh-distributed random variable \( R \), we start with a uniform random variable $U_1$ on $(0, 1)$ and use the **inverse transform method**. 

- The cumulative distribution function (CDF) for the Rayleigh distribution is:

  $F_R(r) = 1 - e^{-r^2 / 2}.$
  
- By setting $F_R(r) = U_1$, we can solve for $r$ in terms of $U_1$:

  $1 - e^{-r^2 / 2} = U_1.$
  
  Rearranging, we find:

  $r = \sqrt{-2 \ln(U_1)}.$
  
This formula, $R = \sqrt{-2 \ln(U_1)}$, transforms the uniform random variable $U_1$ into a Rayleigh-distributed variable, 
which is essential for producing standard normal samples with the Box-Muller method.

### 3. Using Polar Coordinates
Once we have $R$ (the radius) and a uniformly distributed angle $\theta = 2 \pi U_2$, we can use polar-to-Cartesian conversions to get two independent standard normal variables:

$X = R \cos(\theta) \quad \text{and} \quad Y = R \sin(\theta).$

### 4. Why $U_1$ Works for the Exponential Transformation
The logarithmic transformation, $-\ln(U_1)$, works specifically because $U_1$ is uniformly distributed on $(0, 1)$. 
Uniform random variables map well through functions like $-\ln(U_1)$, which is commonly used to generate exponentially distributed random variables. 
In this case, the transformation gives us a Rayleigh distribution, which we need to construct normal distributions in 2D space.

### Summary
Using $U_1$ in the Box-Muller transformation leverages the inverse transform method, turning uniform randomness into Rayleigh-distributed distances. 
This, combined with an angle generated from $U_2$, completes the transformation, resulting in standard normal samples $X$ and $Y$.


   ![image](https://github.com/user-attachments/assets/cb31d3b2-03b0-4028-b7d0-9f45971b1207)





