# Q1
What are the key points in Lecture 1105?

In Lecture 1105, the primary focus was on **functions of jointly distributed random variables**, specifically within the context of **Chapter 3.6: Joint Distributions**. Here are the main topics discussed:

- **Transformation of Variables**: The lecture introduced transformations for jointly continuous random variables, providing tools to change variables and compute new joint distributions. This is crucial when moving from one coordinate system to another (e.g., Cartesian to polar coordinates) in statistical analysis.
  
- **Jacobian Determinant**: The Jacobian matrix and its determinant were used to facilitate transformations of random variables. The determinant of the Jacobian accounts for how areas (or volumes, in higher dimensions) scale under the transformation, ensuring that probabilities remain consistent.

- **Application to Joint Densities**: By using the Jacobian determinant, the lecture demonstrated how to find joint densities in transformed coordinates. For instance, converting jointly normal random variables \(X\) and \(Y\) to polar coordinates resulted in a different joint density function in terms of \(R\) (radius) and \(\theta\) (angle).

- **Examples**: The lecture presented examples that applied these transformations, showing how to calculate joint densities for transformed variables, which is useful for deriving new distributions or simplifying complex integrals in probability.

These key points provide the foundation for understanding transformations and applications in the context of joint distributions.

# Q2
Explain what is the Leibniz integration rule

The **Leibniz rule** is a method for differentiating an integral with respect to a parameter when the limits of integration also depend on the parameter. 
It is commonly used when differentiating functions defined by integrals, especially in probability and statistics.

For an integral of the form:

$$
\int_{a(z)}^{b(z)} f(z, u) \, du,
$$

where $z$ is a parameter, the Leibniz rule states:

$$
\frac{\partial}{\partial z} \left( \int_{a(z)}^{b(z)} f(z, u) \, du \right) = f(z, b(z)) \frac{d}{dz} b(z) - f(z, a(z)) \frac{d}{dz} a(z) + \int_{a(z)}^{b(z)} \frac{\partial}{\partial z} f(z, u) \, du.
$$

### Explanation of Each Term
1. **Boundary Terms** $f(z, b(z)) \frac{d}{dz} b(z) - f(z, a(z)) \frac{d}{dz} a(z)$:
   These account for the fact that the limits of integration, $a(z)$ and $b(z)$, vary with $z$.
3. **Integral of Partial Derivative** $\int_{a(z)}^{b(z)} \frac{\partial}{\partial z} f(z, u) \, du$:
   This term captures the rate of change of the integrand with respect to $z$ over the range of integration.

### Applications
The Leibniz rule is particularly useful in probability and calculus when dealing with cumulative distribution functions (CDFs), 
expected values, and other quantities where integrals depend on parameters.

# Q3
Identify the bivariate normal distribution on page 14 of the slides in class

The bivariate normal distribution identified on page 14 is characterized by the following joint density function:

$$
f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2 \pi \sigma_{Y_1} \sigma_{Y_2} \sqrt{1 - \rho^2}} \exp \left( -\frac{1}{2(1 - \rho^2)} \left( \frac{(y_1 - \mu_{Y_1})^2}{\sigma_{Y_1}^2} + \frac{(y_2 - \mu_{Y_2})^2}{\sigma_{Y_2}^2} - \frac{2 \rho (y_1 - \mu_{Y_1})(y_2 - \mu_{Y_2})}{\sigma_{Y_1} \sigma_{Y_2}} \right) \right).
$$

In this particular example:
- **Means**: $\mu_{Y_1} = 0$ and $\mu_{Y_2} = 0$,
- **Standard Deviations**: $\sigma_{Y_1} = 1$ and $\sigma_{Y_2} = 2$,
- **Correlation**: $\rho = \frac{1}{2}$.

Thus, the distribution is:

$$
(Y_1, Y_2) \sim N_2 \left( \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & 2 \end{pmatrix} \right),
$$

where $N_2$ denotes the bivariate normal distribution.

# Q4
Inverse Transform Method
The **inverse transform method** leverages the cumulative distribution function (CDF) of the desired distribution. 
By applying the inverse CDF (quantile function) of the standard normal distribution to uniformly distributed random variables, 
we obtain samples that follow the standard normal distribution.

### Steps and Code
1. **Generate Uniform Samples**: We start by drawing 1000 samples from the uniform distribution $U(0, 1)$.
2. **Apply the Inverse CDF**: For each sample, we apply the inverse CDF (also called the percent-point function) of the standard normal distribution.
   This transforms uniform samples into standard normal samples.

   ![image](https://github.com/user-attachments/assets/96dbd63e-2eb8-4092-b35b-abad2fba2d30)

# Q5
The **polar method**, or **Box-Muller transform**, is another way to generate pairs of independent standard normal samples. 
This method uses pairs of uniform random variables to produce normal samples through trigonometric transformations.

### Steps and Code

1. **Generate Pairs of Uniform Variables**: Draw two independent uniform random variables, $U_1$ and $U_2$.

2. **Calculate Radius and Angle**:
   - Compute $R = \sqrt{-2 \ln(U_1)}$.
   - Compute $\theta = 2 \pi U_2$.

3. **Calculate Normal Samples**:
   - Set $X = R \cos(\theta)$ and $Y = R \sin(\theta)$.
   - These values will each follow a standard normal distribution.

4. **Repeat**: Continue until we have 1000 total samples.

   ![image](https://github.com/user-attachments/assets/cb31d3b2-03b0-4028-b7d0-9f45971b1207)





