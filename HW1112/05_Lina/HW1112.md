## Problem 43: Maximum Likelihood Estimation

![image](https://github.com/user-attachments/assets/502789a6-ab94-43e2-854c-1d32c6e980f3)


### **Solution**  

1. **Likelihood Function:**  
   The likelihood function is given by:

   $L(\theta) = \prod_{i=1}^n f(x_i; \theta) = \prod_{i=1}^n \theta x_i^{\theta - 1}.$

   Simplifying:

   $L(\theta) = \theta^n \prod_{i=1}^n x_i^{\theta - 1}.$

1. **Log-Likelihood Function:**  
   Take the natural logarithm:

   $\ell(\theta) = \ln L(\theta) = n \ln \theta + (\theta - 1) \sum_{i=1}^n \ln x_i.$

3. **First Derivative:**  
   Differentiate with respect to $\theta$:

   $\frac{\partial \ell(\theta)}{\partial \theta} = \frac{n}{\theta} + \sum_{i=1}^n \ln x_i.$

5. **Solve for $\theta$:**  
   Setting $\frac{\partial \ell(\theta)}{\partial \theta} = 0$:

   $\frac{n}{\theta} + \sum_{i=1}^n \ln x_i = 0.$

   Rearranging:

   $\theta = -\frac{n}{\sum_{i=1}^n \ln x_i}.$

7. **Check Conditions for Maximum:**  
   The second derivative is:

   $\frac{\partial^2 \ell(\theta)}{\partial \theta^2} = -\frac{n}{\theta^2}.$
   Since $\frac{\partial^2 \ell(\theta)}{\partial \theta^2} < 0$ for $\theta > 0$,  $\ell(\theta)$ achieves a maximum.

9. **MLE of $\theta$:**  
   The maximum likelihood estimator is:

   $\hat{\theta} = -\frac{n}{\sum_{i=1}^n \ln x_i}.$

---

## Problem 44: Method of Moments Estimation

![image](https://github.com/user-attachments/assets/96e555e4-c964-45fe-bd28-317f868ba93a)


### **Solution**

1. **First Moment of the Distribution:**  
   The population mean (first moment) is:

   $E(X) = \int_0^1 x \cdot \theta x^{\theta - 1} \, dx.$

   Simplify the integral:

   $E(X) = \theta \int_0^1 x^\theta \, dx = \theta \left[ \frac{x^{\theta + 1}}{\theta + 1} \right]_0^1 = \frac{\theta}{\theta + 1}.$

3. **Sample Mean:**  
   The sample mean is:

   $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i.$

5. **Equating Population and Sample Means:**  
   Using the method of moments, set $E(X) = \bar{X}$:

   $\frac{\theta}{\theta + 1} = \bar{X}.$

7. **Solve for $\theta$:**  
   Rearrange to isolate $\theta$:

   $\theta = \frac{\bar{X}}{1 - \bar{X}}.$

9. **Method of Moments Estimator:**  
   The method of moments estimator is:

   $\hat{\theta}_{\text{MM}} = \frac{\bar{X}}{1 - \bar{X}},$
   where $\bar{X}$ is the sample mean.

---

## Problem 50: Frequency Function of \(X + Y\)

![image](https://github.com/user-attachments/assets/f2ebeb5d-6d7d-44bc-aa71-26c6b8b0566a)


### **Solution**  

1. **Possible Values of \(S\):**  
   Since $X$ and $Y$ take values $(0, 1, 2)$, the sum $(S = X + Y)$ can take values:
   
   $S \in \{0, 1, 2, 3, 4\}.$

2. **Finding the Probability Mass Function (PMF):**  
   Use the independence of $X$ and $Y$ to compute the probabilities for each value of $S$:  
   
   $P(S = k) = \sum_{x} P(X = x)P(Y = k - x),$
     
   where $P(Y = k - x)$ is zero if $(k - x)$ is not in $\{0, 1, 2\}$.

   - For $S = 0$:  
     
     $P(S = 0) = P(X = 0)P(Y = 0) = \frac{1}{3} \cdot \frac{1}{3} = \frac{1}{9}.$

   - For $S = 1$:  
     
     $P(S = 1) = P(X = 0)P(Y = 1) + P(X = 1)P(Y = 0) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{2}{9}.$

   - For $S = 2$:  
     
     $P(S = 2) = P(X = 0)P(Y = 2) + P(X = 1)P(Y = 1) + P(X = 2)P(Y = 0),$
     
     $P(S = 2) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{3}{9} = \frac{1}{3}.$

   - For $S = 3$:  
     
     $P(S = 3) = P(X = 1)P(Y = 2) + P(X = 2)P(Y = 1) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{2}{9}.$

   - For $S = 4$:  
     
     $P(S = 4) = P(X = 2)P(Y = 2) = \frac{1}{3} \cdot \frac{1}{3} = \frac{1}{9}.$

4. **Frequency Function of $S$:**  
   The PMF of $S$ is:

   $$
   P(S = k) = 
   \begin{cases} 
   \frac{1}{9}, & \text{if } k = 0 \text{ or } k = 4, \\
   \frac{2}{9}, & \text{if } k = 1 \text{ or } k = 3, \\
   \frac{1}{3}, & \text{if } k = 2.
   \end{cases}
   $$

---

## Problem 51: Density Function of $Z = XY$

![image](https://github.com/user-attachments/assets/505d34b4-e7cd-4dcb-b361-ce2d89c12915)


### **Solution**

1. **Transformation:**  
   Let $Z = XY$, where $X$ and $Y$ are continuous random variables. For a given value of $z$, solve $z = xy$ for $x$:
   
   $x = \frac{z}{y}.$

2. **Joint PDF:**  
   The joint density $f(x, y)$ can be used to determine the marginal density of $Z$.

3. **Jacobian Transformation:**  
   The transformation involves $z = xy$ and $y = y$. The Jacobian determinant is:
   
   $J = \left|\frac{\partial(x, y)}{\partial(z, y)}\right| = \left|\frac{1}{y}\right|.$

4. **Integrating Out $y$:**  
   The marginal density of $Z$ is obtained by integrating over all possible values of $y$:
   
   $f_Z(z) = \int_{-\infty}^\infty f\left(y, \frac{z}{y}\right) \frac{1}{|y|} \, dy.$

This completes the derivation.

---

## Problem 52: Density of the Quotient of Two Independent Uniform Random Variables

![image](https://github.com/user-attachments/assets/c65ce56a-0bf5-4e95-afcb-dbead319d25d)

## Solution

### Step 1: Joint PDF of $X_1$ and $X_2$
Since $X_1$ and $X_2$ are independent and uniformly distributed over $[0, 1]$, their joint probability density function (PDF) is:

$f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) \cdot f_{X_2}(x_2) = 1 \cdot 1 = 1, \quad 0 \leq x_1 \leq 1, \; 0 \leq x_2 \leq 1.$

---

### Step 2: Transformation of Variables
Let:

$u = \frac{X_1}{X_2}, \quad v = X_2.$

The limits of $u$ and $v$ are determined as follows:
- When $X_1 = 0$, $u = 0$.
- When $X_1 = X_2$, $u = 1$.
- When $X_2 = 0$, $u \to \infty$.
- $v$ retains the range of $X_2$: $0 \leq v \leq 1$.

Thus:

$u \in [0, \infty), \quad v \in [0, 1].$

---

### Step 3: Jacobian of Transformation
The transformation is:

$x_1 = uv, \quad x_2 = v.$

The Jacobian determinant is:

$$
\left| J \right| = \left| 
\begin{vmatrix}
\frac{\partial x_1}{\partial u} & \frac{\partial x_1}{\partial v} \\
\frac{\partial x_2}{\partial u} & \frac{\partial x_2}{\partial v}
\end{vmatrix} 
\right| = \left| 
\begin{vmatrix}
v & u \\
0 & 1
\end{vmatrix} 
\right| = v.
$$

---

### Step 4: Joint PDF of $U$ and $V$
The joint PDF of $U$ and $V$ is:

$g(u, v) = f_{X_1, X_2}(uv, v) \cdot \left| J \right| = 1 \cdot v = v, \quad 0 \leq v \leq 1, \; u \in [0, \infty).$

![image](https://github.com/user-attachments/assets/edb7ab51-4067-42bd-a3b0-03c265601a29)

---

### Step 5: Marginal PDF of $U$
To find the PDF of $U$, integrate out $v$ from the joint PDF:

$f_U(u) = \int_0^1 g(u, v) \, dv = \int_0^1 v \, dv.$ 

The computation depends on the value of $u$:
- **Region I $( 0 \leq u \leq 1)$:**
  
  $f_U(u) = \int_0^u v \, dv = \left[ \frac{v^2}{2} \right]_0^u = \frac{u^2}{2}.$

- **Region II $(u > 1)$:**
  
  $f_U(u) = \int_0^{\frac{1}{u}} v \, dv = \left[ \frac{v^2}{2} \right]_0^{\frac{1}{u}} = \frac{1}{2u^2}.$

---

### Step 6: Final PDF of $U$
The PDF of $U = \frac{X_1}{X_2}$ is:

$$
f_U(u) = 
\begin{cases} 
\frac{u}{2}, & \text{if } 0 \leq u \leq 1, \\
\frac{1}{2u^2}, & \text{if } u > 1.
\end{cases}
$$

---

## Final Answer
The density function of $U = \frac{X_1}{X_2}$ is:

$$
f_U(u) = 
\begin{cases} 
\frac{u}{2}, & \text{if } 0 \leq u \leq 1, \\
\frac{1}{2u^2}, & \text{if } u > 1.
\end{cases}
$$



