## Problem 43: Maximum Likelihood Estimation

![image](https://github.com/user-attachments/assets/502789a6-ab94-43e2-854c-1d32c6e980f3)


### **Solution**  

1. **Likelihood Function:**  
   The likelihood function is given by:

   $L(\theta) = \prod_{i=1}^n f(x_i; \theta) = \prod_{i=1}^n \theta x_i^{\theta - 1}.$

   Simplifying:

   $L(\theta) = \theta^n \prod_{i=1}^n x_i^{\theta - 1}.$

1. **Log-Likelihood Function:**  
   Take the natural logarithm:

   $\ell(\theta) = \ln L(\theta) = n \ln \theta + (\theta - 1) \sum_{i=1}^n \ln x_i.$

3. **First Derivative:**  
   Differentiate with respect to $\theta$:

   $\frac{\partial \ell(\theta)}{\partial \theta} = \frac{n}{\theta} + \sum_{i=1}^n \ln x_i.$

5. **Solve for $\theta$:**  
   Setting $\frac{\partial \ell(\theta)}{\partial \theta} = 0$:

   $\frac{n}{\theta} + \sum_{i=1}^n \ln x_i = 0.$

   Rearranging:

   $\theta = -\frac{n}{\sum_{i=1}^n \ln x_i}.$

7. **Check Conditions for Maximum:**  
   The second derivative is:

   $\frac{\partial^2 \ell(\theta)}{\partial \theta^2} = -\frac{n}{\theta^2}.$
   Since $\frac{\partial^2 \ell(\theta)}{\partial \theta^2} < 0$ for $\theta > 0$,  $\ell(\theta)$ achieves a maximum.

9. **MLE of $\theta$:**  
   The maximum likelihood estimator is:

   $\hat{\theta} = -\frac{n}{\sum_{i=1}^n \ln x_i}.$

---

## Problem 44: Method of Moments Estimation

![image](https://github.com/user-attachments/assets/96e555e4-c964-45fe-bd28-317f868ba93a)


### **Solution**

1. **First Moment of the Distribution:**  
   The population mean (first moment) is:

   $E(X) = \int_0^1 x \cdot \theta x^{\theta - 1} \, dx.$

   Simplify the integral:

   $E(X) = \theta \int_0^1 x^\theta \, dx = \theta \left[ \frac{x^{\theta + 1}}{\theta + 1} \right]_0^1 = \frac{\theta}{\theta + 1}.$

3. **Sample Mean:**  
   The sample mean is:

   $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i.$

5. **Equating Population and Sample Means:**  
   Using the method of moments, set $E(X) = \bar{X}$:

   $\frac{\theta}{\theta + 1} = \bar{X}.$

7. **Solve for $\theta$:**  
   Rearrange to isolate $\theta$:

   $\theta = \frac{\bar{X}}{1 - \bar{X}}.$

9. **Method of Moments Estimator:**  
   The method of moments estimator is:

   $\hat{\theta}_{\text{MM}} = \frac{\bar{X}}{1 - \bar{X}},$
   where $\bar{X}$ is the sample mean.

---

## Problem 50: Frequency Function of \(X + Y\)

![image](https://github.com/user-attachments/assets/f2ebeb5d-6d7d-44bc-aa71-26c6b8b0566a)


### **Solution**  

1. **Possible Values of \(S\):**  
   Since $X$ and $Y$ take values $(0, 1, 2)$, the sum $(S = X + Y)$ can take values:
   
   $S \in \{0, 1, 2, 3, 4\}.$

2. **Finding the Probability Mass Function (PMF):**  
   Use the independence of $X$ and $Y$ to compute the probabilities for each value of $S$:  
   
   $P(S = k) = \sum_{x} P(X = x)P(Y = k - x),$
     
   where $P(Y = k - x)$ is zero if $(k - x)$ is not in $\{0, 1, 2\}$.

   - For $S = 0$:  
     
     $P(S = 0) = P(X = 0)P(Y = 0) = \frac{1}{3} \cdot \frac{1}{3} = \frac{1}{9}.$

   - For $S = 1$:  
     
     $P(S = 1) = P(X = 0)P(Y = 1) + P(X = 1)P(Y = 0) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{2}{9}.$

   - For $S = 2$:  
     
     $P(S = 2) = P(X = 0)P(Y = 2) + P(X = 1)P(Y = 1) + P(X = 2)P(Y = 0),$
     
     $P(S = 2) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{3}{9} = \frac{1}{3}.$

   - For $S = 3$:  
     
     $P(S = 3) = P(X = 1)P(Y = 2) + P(X = 2)P(Y = 1) = \frac{1}{3} \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{2}{9}.$

   - For $S = 4$:  
     
     $P(S = 4) = P(X = 2)P(Y = 2) = \frac{1}{3} \cdot \frac{1}{3} = \frac{1}{9}.$

4. **Frequency Function of $S$:**  
   The PMF of $S$ is:

   $$
   P(S = k) = 
   \begin{cases} 
   \frac{1}{9}, & \text{if } k = 0 \text{ or } k = 4, \\
   \frac{2}{9}, & \text{if } k = 1 \text{ or } k = 3, \\
   \frac{1}{3}, & \text{if } k = 2.
   \end{cases}
   $$

---

## Problem 51: Density Function of $Z = XY$

![image](https://github.com/user-attachments/assets/505d34b4-e7cd-4dcb-b361-ce2d89c12915)


### **Solution**

1. **Transformation:**  
   Let $Z = XY$, where $X$ and $Y$ are continuous random variables. For a given value of $z$, solve $z = xy$ for $x$:
   
   $x = \frac{z}{y}.$

2. **Joint PDF:**  
   The joint density $f(x, y)$ can be used to determine the marginal density of $Z$.

3. **Jacobian Transformation:**  
   The transformation involves $z = xy$ and $y = y$. The Jacobian determinant is:
   
   $J = \left|\frac{\partial(x, y)}{\partial(z, y)}\right| = \left|\frac{1}{y}\right|.$

4. **Integrating Out $y$:**  
   The marginal density of $Z$ is obtained by integrating over all possible values of $y$:
   
   $f_Z(z) = \int_{-\infty}^\infty f\left(y, \frac{z}{y}\right) \frac{1}{|y|} \, dy.$

This completes the derivation.

---

## Problem 52: Density of the Quotient of Two Independent Uniform Random Variables

![image](https://github.com/user-attachments/assets/c65ce56a-0bf5-4e95-afcb-dbead319d25d)

## Solution

### Step 1: Joint PDF of $X_1$ and $X_2$
Since $X_1$ and $X_2$ are independent and uniformly distributed over $[0, 1]$, their joint probability density function (PDF) is:

$f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) \cdot f_{X_2}(x_2) = 1 \cdot 1 = 1, \quad 0 \leq x_1 \leq 1, \; 0 \leq x_2 \leq 1.$

---

### Step 2: Transformation of Variables
Let:

$u = \frac{X_1}{X_2}, \quad v = X_2.$

The limits of $u$ and $v$ are determined as follows:
- When $X_1 = 0$, $u = 0$.
- When $X_1 = X_2$, $u = 1$.
- When $X_2 = 0$, $u \to \infty$.
- $v$ retains the range of $X_2$: $0 \leq v \leq 1$.

Thus:

$u \in [0, \infty), \quad v \in [0, 1].$

---

### Step 3: Jacobian of Transformation
The transformation is:

$x_1 = uv, \quad x_2 = v.$

The Jacobian determinant is:

$$
\left| J \right| = \left| 
\begin{vmatrix}
\frac{\partial x_1}{\partial u} & \frac{\partial x_1}{\partial v} \\
\frac{\partial x_2}{\partial u} & \frac{\partial x_2}{\partial v}
\end{vmatrix} 
\right| = \left| 
\begin{vmatrix}
v & u \\
0 & 1
\end{vmatrix} 
\right| = v.
$$

---

### Step 4: Joint PDF of $U$ and $V$
The joint PDF of $U$ and $V$ is:

$g(u, v) = f_{X_1, X_2}(uv, v) \cdot \left| J \right| = 1 \cdot v = v, \quad 0 \leq v \leq 1, \; u \in [0, \infty).$

![image](https://github.com/user-attachments/assets/edb7ab51-4067-42bd-a3b0-03c265601a29)

---

### Step 5: Marginal PDF of $U$
To find the PDF of $U$, integrate out $v$ from the joint PDF:

$f_U(u) = \int_0^1 g(u, v) \, dv = \int_0^1 v \, dv.$ 

The computation depends on the value of $u$:
- **Region I $( 0 \leq u \leq 1)$:**
  
  $f_U(u) = \int_0^u v \, dv = \left[ \frac{v^2}{2} \right]_0^u = \frac{u^2}{2}.$

- **Region II $(u > 1)$:**
  
  $f_U(u) = \int_0^{\frac{1}{u}} v \, dv = \left[ \frac{v^2}{2} \right]_0^{\frac{1}{u}} = \frac{1}{2u^2}.$

---

### Step 6: Final PDF of $U$
The PDF of $U = \frac{X_1}{X_2}$ is:

$$
f_U(u) = 
\begin{cases} 
\frac{u}{2}, & \text{if } 0 \leq u \leq 1, \\
\frac{1}{2u^2}, & \text{if } u > 1.
\end{cases}
$$

---

## Final Answer
The density function of $U = \frac{X_1}{X_2}$ is:

$$
f_U(u) = 
\begin{cases} 
\frac{u}{2}, & \text{if } 0 \leq u \leq 1, \\
\frac{1}{2u^2}, & \text{if } u > 1.
\end{cases}
$$

---

# Problem 54: Joint and Marginal Densities of \(\Theta\), \(\Phi\), and \(R\)

![image](https://github.com/user-attachments/assets/e3fda19f-fce9-48e7-864d-8aeb121fcb87)

## Solution

### Step 1: Transformation from Cartesian to Spherical Coordinates
The spherical coordinates are related to the Cartesian coordinates by:

$r = \sqrt{x^2 + y^2 + z^2}, \quad \phi = \arccos\left(\frac{z}{r}\right), \quad \theta = \arctan\left(\frac{y}{x}\right).$

The volume element transformation is:

$dx\,dy\,dz = r^2 \sin\phi \, dr\,d\theta\,d\phi.$

---

### Step 2: Joint Density of $X, Y, Z$
The joint PDF of $X$, $Y$, and $Z$, since they are independent $N(0, \sigma^2)$, is:

$f_{X,Y,Z}(x, y, z) = f_X(x) f_Y(y) f_Z(z) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^3 \exp\left(-\frac{x^2 + y^2 + z^2}{2\sigma^2}\right).$

---

### Step 3: Transform to $(\Theta, \Phi, R)$
Substituting $x^2 + y^2 + z^2 = r^2$, the joint PDF of $(R, \Phi, \Theta)$ becomes:

$f_{R,\Phi,\Theta}(r, \phi, \theta) = f_{X,Y,Z}(x, y, z) \cdot \left| J \right|,$

where $\left| J \right| = r^2 \sin\phi$ is the Jacobian determinant.

Thus:

$f_{R,\Phi,\Theta}(r, \phi, \theta) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^3 \exp\left(-\frac{r^2}{2\sigma^2}\right) r^2 \sin\phi.$

---

### Step 4: Marginal Densities

#### 1. Marginal Density of $R$
Integrate over the ranges of $\phi \in [0, \pi]$ and $\theta \in [0, 2\pi]$:

$f_R(r) = \int_0^{2\pi} \int_0^\pi f_{R,\Phi,\Theta}(r, \phi, \theta) \, \sin\phi \, d\phi \, d\theta.$

Substituting $f_{R,\Phi,\Theta}$:

$f_R(r) = \int_0^{2\pi} \int_0^\pi \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^3 \exp\left(-\frac{r^2}{2\sigma^2}\right) r^2 \sin\phi \, d\phi \, d\theta.$

The integration over $\phi$ and $\theta$ yields:

$f_R(r) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^3 \cdot 4\pi \cdot r^2 \exp\left(-\frac{r^2}{2\sigma^2}\right).$

Simplify:

$f_R(r) = \frac{r^2}{\sigma^3 \sqrt{2\pi}} \exp\left(-\frac{r^2}{2\sigma^2}\right), \quad r \geq 0.$

This is the PDF of the Rayleigh distribution.

---

#### 2. Marginal Density of $\Phi$
Since $\Phi$ is independent of $R$ and $\Theta$, its distribution is uniform over $[0, \pi]$:

$f_\Phi(\phi) = \frac{1}{\pi}, \quad 0 \leq \phi \leq \pi.$

---

#### 3. Marginal Density of \(\Theta\)
Similarly, $\Theta$ is uniformly distributed over $[0, 2\pi]$:

$f_\Theta(\theta) = \frac{1}{2\pi}, \quad 0 \leq \theta < 2\pi.$

---

# Problem 59 and 60: Bivariate Normal Distribution and Pseudorandom Variable Generation

---

## Problem 59: Joint Distribution of \( Y_1 \) and \( Y_2 \)

**Problem Statement**  
![image](https://github.com/user-attachments/assets/7d1cb760-934d-4f80-ac68-c486faf07642)

### Solution

### Step 1: Linear Transformation of Normals
- $X_1$ and $X_2$ are independent $N(0, 1)$.  
- Any linear combination of independent normal random variables is also normally distributed.  
- Therefore, $Y_1$ and $Y_2$, which are linear combinations of $X_1$ and $X_2$, are jointly normal.

---

### Step 2: Mean Vector
The means of $Y_1$ and $Y_2$ are:

$\mu_{Y_1} = E[Y_1] = E[a_{11}X_1 + a_{12}X_2 + b_1] = b_1,$


$\mu_{Y_2} = E[Y_2] = E[a_{21}X_1 + a_{22}X_2 + b_2] = b_2.$

Thus, the mean vector is:

$$
\mathbf{\mu} = 
\begin{bmatrix}
b_1 \\
b_2
\end{bmatrix}.
$$

---

### Step 3: Covariance Matrix
The covariance between $Y_1$ and $Y_2$ is:

$\text{Cov}(Y_1, Y_2) = E[(Y_1 - \mu_{Y_1})(Y_2 - \mu_{Y_2})].$

Substitute $Y_1$ and $Y_2$:

$\text{Cov}(Y_1, Y_2) = E\left[(a_{11}X_1 + a_{12}X_2)(a_{21}X_1 + a_{22}X_2)\right].$

Expanding and using the independence of $X_1$ and $X_2$ $( E[X_1 X_2] = 0)$:

$\text{Cov}(Y_1, Y_2) = a_{11}a_{21}E[X_1^2] + a_{12}a_{22}E[X_2^2] = a_{11}a_{21} + a_{12}a_{22}.$

The variances of $Y_1$ and $Y_2$ are:

$\text{Var}(Y_1) = E[(Y_1 - \mu_{Y_1})^2] = a_{11}^2 + a_{12}^2,$

$\text{Var}(Y_2) = E[(Y_2 - \mu_{Y_2})^2] = a_{21}^2 + a_{22}^2.$

Thus, the covariance matrix is:

$$
\Sigma = 
\begin{bmatrix}
a_{11}^2 + a_{12}^2 & a_{11}a_{21} + a_{12}a_{22} \\
a_{11}a_{21} + a_{12}a_{22} & a_{21}^2 + a_{22}^2
\end{bmatrix}.
$$

---

### Step 4: Conclusion
Since $Y_1$ and $Y_2$ are linear combinations of independent normals, and we computed the covariance matrix, $Y_1$ and $Y_2$ follow a **bivariate normal distribution**:

$$
\mathbf{Y} = 
\begin{bmatrix}
Y_1 \\
Y_2
\end{bmatrix}
\sim N(\mathbf{\mu}, \Sigma).
$$

---

## Problem 60: Generating Bivariate Normal Variables

![image](https://github.com/user-attachments/assets/e2097c4a-57a6-4afd-bee6-f1de9bbce7e2)

### Solution

### Step 1: Generate Independent Standard Normals
1. Use two independent pseudorandom uniform variables $U_1$ and $U_2$ in $[0, 1]$.
2. Transform $U_1$ and $U_2$ into independent standard normal random variables $X_1$ and $X_2$ using the **Box-Muller transformation**:
   
   $X_1 = \sqrt{-2\ln U_1} \cos(2\pi U_2), \quad X_2 = \sqrt{-2\ln U_1} \sin(2\pi U_2).$

---

### Step 2: Linear Transformation
Given the bivariate normal distribution:

$Y_1 = a_{11}X_1 + a_{12}X_2 + b_1, \quad Y_2 = a_{21}X_1 + a_{22}X_2 + b_2,$

transform $X_1$ and $X_2$ into $Y_1$ and $Y_2$ using the specified coefficients $a_{ij}$ and constants $b_1, b_2$.

---

### Step 3: Resulting Bivariate Normal Distribution
The transformed variables $Y_1$ and $Y_2$ will follow the bivariate normal distribution:

$$
\begin{bmatrix}
Y_1 \\
Y_2
\end{bmatrix}
\sim N\left(
\begin{bmatrix}
b_1 \\
b_2
\end{bmatrix}, 
\begin{bmatrix}
a_{11}^2 + a_{12}^2 & a_{11}a_{21} + a_{12}a_{22} \\
a_{11}a_{21} + a_{12}a_{22} & a_{21}^2 + a_{22}^2
\end{bmatrix}
\right).
$$




